{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "search_query = \"What are your refund policies?\"\n",
    "search_query = re.sub(r'\\bDaraz\\b\\s*', '', search_query, flags=re.IGNORECASE)\n",
    "file_name = \"DarazDataMain.txt\"\n",
    "chunkSize = 1500\n",
    "\n",
    "# %%\n",
    "# Load the data from the text file\n",
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()\n",
    "\n",
    "# %%\n",
    "# Initialize the sentence transformer model for generating embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each product\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are your refund policies?\n",
      "\n",
      "Score: 0.5706478953361511\n",
      "Product Details: 1.19 “Returns and Refunds Policy” shall mean the applicable Company policies which govern the procedure for returns and refunds of Products by Customers on the relevant Channels located at Returns and Refunds Policy of Pakistan.\n",
      "\n",
      "\n",
      "Score: 0.549439549446106\n",
      "Product Details: Home & Living Bedding & Bath, Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements, Household & Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind isnot applicable for return and refund. If the item received is damaged, defective, incorrect, or incomplete, a refund will be issued based on Daraz's assessment. Note: For device-related issues after usage or expiration of return policy period, please check if the item is covered under Seller orBrand Warranty. Refer to our Warranty Policy for information on the different warranty types and ways to contact the seller/manufacturer. Items that are non-returnable: Any custom-made items\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Define a function to search for similar products\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "# %%\n",
    "search_products(search_query, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = search_query\n",
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 26958, which is longer than the specified 1500\n",
      "Created a chunk of size 2500, which is longer than the specified 1500\n",
      "Created a chunk of size 2032, which is longer than the specified 1500\n",
      "Created a chunk of size 1537, which is longer than the specified 1500\n",
      "Created a chunk of size 1662, which is longer than the specified 1500\n",
      "Created a chunk of size 2178, which is longer than the specified 1500\n",
      "Created a chunk of size 1608, which is longer than the specified 1500\n",
      "Created a chunk of size 2486, which is longer than the specified 1500\n",
      "Created a chunk of size 2510, which is longer than the specified 1500\n",
      "Created a chunk of size 1754, which is longer than the specified 1500\n",
      "Created a chunk of size 3013, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chroma/' + str(file_name) + \" - Chunker = \" + chunker01\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chroma/' + str(file_name) + \" - Chunker = \" + chunker02\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# chunker03 = \"Sentence splitter\"\n",
    "\n",
    "# def tokenize_and_chunk(text, spacy_model=\"en_core_web_sm\", overlap=0, stride=1):\n",
    "#     nlp = spacy.load(spacy_model)\n",
    "#     sentences = list(nlp(text).sents)\n",
    "#     chunks = []\n",
    "\n",
    "#     for i in range(0, len(sentences), stride):\n",
    "#         chunk_text = \" \".join(str(sent) for sent in sentences[i:i + overlap + 1])\n",
    "#         chunks.append(chunk_text)\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Now using the 'page_content' attribute to get the document text\n",
    "# splits = [tokenize_and_chunk(doc.page_content) for doc in docs]\n",
    "\n",
    "# # Assuming `file_name` is defined earlier in your code.\n",
    "# persist_directory = 'chroma/' + str(file_name) + \" - Chunker = \" + chunker03\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=GPT4AllEmbeddings(),\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "# vectordb.persist()\n",
    "# docs03 = vectordb.similarity_search_with_score(query, k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker03 = \"Structure Aware Splitting (by Sentence, Paragraph)\"\n",
    "\n",
    "# splits = docs.split(\".\")\n",
    "\n",
    "# persist_directory = 'chroma/' + str(file_name) + \" - Chunker = \" + chunker03\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=GPT4AllEmbeddings(),\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "# vectordb.persist()\n",
    "# docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2157, which is longer than the specified 1500\n",
      "Created a chunk of size 1692, which is longer than the specified 1500\n",
      "Created a chunk of size 1967, which is longer than the specified 1500\n",
      "Created a chunk of size 1830, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker04 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chroma/' + str(file_name) + \" - Chunker = \" + chunker04\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs04 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "# print(\"\\n\" + chunker01)\n",
    "# count = 1\n",
    "# for result in docs01:\n",
    "#     print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "#     count+=1\n",
    "\n",
    "\n",
    "# print(\"\\n\" + chunker02)\n",
    "# count = 1\n",
    "# for result in docs02:\n",
    "#     print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "#     count+=1\n",
    "\n",
    "# print(\"\\n\" + chunker03)\n",
    "# count = 1\n",
    "# for result in docs03:\n",
    "#     print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "#     count+=1\n",
    "\n",
    "# print(\"\\n\" + chunker04)\n",
    "# count = 1\n",
    "# for result in docs04:\n",
    "#     print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "#     count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"What are your refund policies?\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "# Open a file to write the output\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    # file.write(f\"question = \\\"{query}\\\"\\n\")\n",
    "\n",
    "    # Handle the output for chunker01\n",
    "    # file.write(f\"\\n{chunker01}\\n\")\n",
    "    count = 1\n",
    "    for result in docs01:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    # Handle the output for chunker02\n",
    "    # file.write(f\"\\n{chunker02}\\n\")\n",
    "    count = 1\n",
    "    for result in docs02:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    # # Handle the output for chunker03\n",
    "    # file.write(f\"\\n{chunker03}\\n\")\n",
    "    # count = 1\n",
    "    # for result in docs03:\n",
    "    #     file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "    #     count += 1\n",
    "\n",
    "    # Handle the output for chunker04\n",
    "    # file.write(f\"\\n{chunker04}\\n\")\n",
    "    count = 1\n",
    "    for result in docs04:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned and saved to 'output.txt'\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import textwrap\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Remove any image file references\n",
    "#     text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "\n",
    "#     # Normalize spacing issues\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "#     # Correct common typographical errors\n",
    "#     text = re.sub(r\"isnot\", \"is not\", text)\n",
    "#     text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "#     text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "\n",
    "#     # Remove redundant response indicators\n",
    "#     text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "\n",
    "#     # Deduplicate text\n",
    "#     lines = text.split('.')\n",
    "#     seen = set()\n",
    "#     unique_lines = []\n",
    "#     for line in lines:\n",
    "#         line_clean = line.strip()\n",
    "#         if line_clean not in seen:\n",
    "#             seen.add(line_clean)\n",
    "#             unique_lines.append(line_clean)\n",
    "\n",
    "#     # Reconstruct text with clean lines\n",
    "#     cleaned_text = '. '.join(unique_lines).strip()\n",
    "#     if not cleaned_text.endswith('.'):\n",
    "#         cleaned_text += '.'\n",
    "\n",
    "#     # Split into multiple lines with a reasonable width\n",
    "#     wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "#     return wrapped_text\n",
    "\n",
    "# # Example usage with a read from a file and clean\n",
    "# with open('output.txt', 'r') as file:\n",
    "#     raw_text = file.read()\n",
    "\n",
    "# cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# # Optionally, write the cleaned text back to the file or use it directly\n",
    "# with open('output.txt', 'w') as file:\n",
    "#     file.write(cleaned_text)\n",
    "\n",
    "# print(\"Text cleaned and saved to 'output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned and saved to 'output.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove any image file references\n",
    "    text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "\n",
    "    # Normalize spacing issues\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Correct common typographical errors\n",
    "    text = re.sub(r\"isnot\", \"is not\", text)\n",
    "    text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "    text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "\n",
    "    # Remove redundant response indicators\n",
    "    text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed by a dot, e.g., \"1.\"\n",
    "    text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed directly by a dash, e.g., \"2-\"\n",
    "    text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "    # Deduplicate text\n",
    "    lines = text.split('.')\n",
    "    seen = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        if line_clean not in seen:\n",
    "            seen.add(line_clean)\n",
    "            unique_lines.append(line_clean)\n",
    "\n",
    "    # Reconstruct text with clean lines\n",
    "    cleaned_text = '. '.join(unique_lines).strip()\n",
    "    if not cleaned_text.endswith('.'):\n",
    "        cleaned_text += '.'\n",
    "\n",
    "    # Split into multiple lines with a reasonable width\n",
    "    wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "# Example usage with a read from a file and clean\n",
    "with open('output.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# Optionally, write the cleaned text back to the file or use it directly\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(\"Text cleaned and saved to 'output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are your refund policies?\n",
      "\n",
      "Score: 0.18136615\n",
      "Product Details: Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind is not applicable for return\n",
      "\n",
      "Score: 0.17416027\n",
      "Product Details: overseas). For overseas products, please refer to the product page to check the applicable return\n"
     ]
    }
   ],
   "source": [
    "# # %%\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import gensim\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# search_query = \"What are your refund policies?\"\n",
    "# search_query = re.sub(r'\\bDaraz\\b\\s*', '', search_query, flags=re.IGNORECASE)\n",
    "# file_name = \"DarazDataMain.txt\"\n",
    "# chunkSize = 1500\n",
    "\n",
    "# # %%\n",
    "# # Load the data from the text file\n",
    "# with open(\"output.txt\", 'r', encoding=\"utf8\") as f:\n",
    "#     products = f.read().split('\\n')\n",
    "\n",
    "# # %%\n",
    "# # Prepare the documents for training\n",
    "# documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(products)]\n",
    "\n",
    "# # Train a Doc2Vec model\n",
    "# model = Doc2Vec(documents, vector_size=100, min_count=1, workers=4, epochs=5)\n",
    "# # model = Doc2Vec(documents, vector_size=50, min_count=1, workers=4, epochs=5)\n",
    "\n",
    "# # %%\n",
    "# # Define a function to search for similar products\n",
    "# def search_products(query, k):\n",
    "#     # Generate query embedding\n",
    "#     query_embedding = model.infer_vector(query.split())\n",
    "    \n",
    "#     # Generate embeddings for all products\n",
    "#     product_embeddings = np.array([model.infer_vector(doc.words) for doc in documents])\n",
    "    \n",
    "#     # Calculate cosine similarities between the query and all product embeddings\n",
    "#     similarities = cosine_similarity([query_embedding], product_embeddings)[0]\n",
    "    \n",
    "#     # Get the top k most similar products\n",
    "#     top_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "#     print(\"Query:\", query)\n",
    "#     for index in top_indices:\n",
    "#         print(\"\\nScore:\", similarities[index])\n",
    "#         print(\"Product Details:\", ' '.join(documents[index].words))\n",
    "\n",
    "# # %%\n",
    "# search_products(search_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are your refund policies?\n",
      "\n",
      "Score: 0.6342750787734985\n",
      "Product Details: policy? Refund against Return orders: the refund is processed if your return claim is deemed valid.\n",
      "\n",
      "\n",
      "Score: 0.5977258682250977\n",
      "Product Details: and refund. Items that are non-returnable: Any custom-made items\" response1 = Does Daraz refund\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # %%\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import re\n",
    "\n",
    "# search_query = \"What are your refund policies?\"\n",
    "# search_query = re.sub(r'\\bDaraz\\b\\s*', '', search_query, flags=re.IGNORECASE)\n",
    "# file_name = \"DarazDataMain.txt\"\n",
    "# chunkSize = 1500\n",
    "\n",
    "# # %%\n",
    "# # Load the data from the text file\n",
    "# with open(\"output.txt\", 'r', encoding=\"utf8\") as f:\n",
    "#     products = f.readlines()\n",
    "\n",
    "# # %%\n",
    "# # Initialize the sentence transformer model for generating embeddings\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Generate embeddings for each product\n",
    "# product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "# # %%\n",
    "# # Define a function to search for similar products\n",
    "# def search_products(query, k):\n",
    "#     query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "#     cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "#     top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "#     print(\"Query:\", query)\n",
    "#     for score, idx in zip(top_results[0], top_results[1]):\n",
    "#         print(\"\\nScore:\", score.item())\n",
    "#         print(\"Product Details:\", products[idx])\n",
    "\n",
    "# # %%\n",
    "# search_products(search_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home & Living Bedding & Bath, Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements,\n",
      "\n",
      "Household & Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind is not applicable\n",
      "\n",
      "for return and refund. If the item received is damaged, defective, incorrect, or incomplete, a\n",
      "\n",
      "refund will be issued based on Daraz's assessment. Note: For device-related issues after usage or\n",
      "\n",
      "expiration of return policy period, please check if the item is covered under Seller or Brand\n",
      "\n",
      "Warranty. Refer to our Warranty Policy for information on the different warranty types and ways to\n",
      "\n",
      "contact the seller/manufacturer. Items that are non-returnable: Any custom-made items\" Home & Living\n",
      "\n",
      "Bedding & Bath, Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements, Household &\n",
      "\n",
      "Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind is not applicable for return\n",
      "\n",
      "and refund. Items that are non-returnable: Any custom-made items\" response1 = Does Daraz refund\n",
      "\n",
      "shipping fee? If the order is canceled. If the order delivery attempt is Failed by the courier. 1-\n",
      "\n",
      "Damaged, defective 2- Expired item 3- Incorrect 4- Not as advertised 5- Counterfeit item 6-\n",
      "\n",
      "Incorrect size 7- Missing items 8- Freebies or accessories 9- Change of mind (if applicable). What\n",
      "\n",
      "is Daraz's refund policy? Refund against Return orders: the refund is processed if your return claim\n",
      "\n",
      "is deemed valid. Refund against Cancelled orders: Refund is automatically triggered once the\n",
      "\n",
      "cancellation has been successfully processed. \" It is recommended to use only original chargers that\n",
      "\n",
      "are accompanied in the box to preserve battery health and prevent overcharging or overheating. Daraz\n",
      "\n",
      "reserves the right to reject or accept warranty claims on a case to case basis. Additionally, Daraz\n",
      "\n",
      "reserves the right to modify the terms and conditions of Daraz Like New Warranty at any time without\n",
      "\n",
      "any notice. Items that are non-returnable: Any custom-made items Sports & Travel Clothing, Apparel,\n",
      "\n",
      "Shoes & Sunglasses Change of mind is applicable for return and refund (excluding non-DarazMall items\n",
      "\n",
      "shipped from overseas). For overseas products, please refer to the product page to check the\n",
      "\n",
      "applicable return reasons. Bags & Luggage, Sport Watches, Team & Racket Sports, Dance & Gymnastics,\n",
      "\n",
      "Exercise & Fitness, Sports Nutrition & Supplements, Outdoor Equipment, Fitness & Other Sports\n",
      "\n",
      "Equipment Change of mind is not applicable for return and refund. Baby, Toys & Kids Clothing,\n",
      "\n",
      "Apparel, Sunglasses, Shoes & Accessories Change of mind is applicable for return and refund\n",
      "\n",
      "(excluding non-DarazMall items shipped from overseas). Toys & Games, Baby Care & Hygiene, Baby Gear,\n",
      "\n",
      "Diapers & Potties, Feeding & Nursing Change of mind is not applicable for return and refund.\n",
      "\n",
      "Grocer's Shop Bakery, Beverages, Baking & Cooking, Cigars & Cigarettes, Dairy, Packaged Foods, Party\n",
      "\n",
      "Accessories, Snacks, Meat & Seafood, Fruits & Vegetables Change of mind is not applicable for return\n",
      "\n",
      "and refund. If the item received is expired, damaged, defective, incorrect, or incomplete, a refund\n",
      "\n",
      "will be issued based on Daraz's assessment. Any 3 Bundle Return request must be raised within 14\n",
      "\n",
      "days for Any 3 Bundle from the date of delivery. Change of Mind is not applicable for return and\n",
      "\n",
      "refund. Partial returns are allowed: If a certain number of products from an Any 3 Bundle are\n",
      "\n",
      "eligible for a return, partial refund will be issued to the customer after these items have been\n",
      "\n",
      "returned by the customer & quality checked by Daraz. The refund amount is calculated on item level\n",
      "\n",
      "of each bundle. The total amount paid for the order will be divided by 3, multiplied by the number\n",
      "\n",
      "of items you have returned. Books & Stationery Change of mind is not applicable for return and\n",
      "\n",
      "refund. \".\n"
     ]
    }
   ],
   "source": [
    "# Read the entire content of the file and save it to the variable 'content'\n",
    "with open('output.txt', 'r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# # Optionally, print the content to verify it's loaded correctly\n",
    "# for line in content:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from transformers.generation import GenerationConfig\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import textwrap\n",
    "\n",
    "# !pip install tiktoken\n",
    "# !pip install tiktoken transformers_stream_generator einops optimum auto-gptq\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# question = \"What are your refund policies?\"\n",
    "# with open('/kaggle/input/ita-proj/output.txt', 'r') as file:\n",
    "#     content = file.readlines()\n",
    "\n",
    "# def get_completion(prompt):\n",
    "#     messages = [{\n",
    "#         \"role\": \"user\", \n",
    "#         \"content\": prompt }]\n",
    "#     prompt2 = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     outputs = pipe(prompt2, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "#     return outputs[0][\"generated_text\"]\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "# Based on the following information:\\n\\n\n",
    "# 1. {content}\\n\\n\n",
    "# Please provide a detailed answer to the question: {question}.\n",
    "# Your answer should integrate the essence of the entire content, providing a unified answer that leverages the \\\n",
    "# diverse perspectives or data points provided by the entire content.\n",
    "# \"\"\"\n",
    "\n",
    "# response = get_completion(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following information:\n",
    "\n",
    "\n",
    "1. Product 12: Product Name = High Quality Wrist Watch For Men & Boys| Decent Wrist Leather Strap Attractive Dial, Product Category = Watches Sunglasses Jewellery/Watches/Men/Fashion, Brand Name = No Brand, Seller Name = Maal-Lo, URL = https://www.daraz.pk/products/-i409485404-s1960964280.html?search=1, Price Details = Original: Rs. 1000, Discounted: Rs. 699 | Original: Rs. 1000, Discounted: Rs. 900 | Original: Rs. 1000, Discounted: Rs. 799 | Original: Rs. 1000, Discounted: Rs. 580 | Original: Rs. 1000, Discounted: Rs. 590 | Original: Rs. 1000, Discounted: Rs. 580, Positive Seller Ratings = 90%, Ship on Time = 98%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
    "\n",
    "\n",
    "2. Product 08: Product Name = OMG's Stylish watch for men , steel Built Design , Heavy Weight Watch in Fashion and for Casual use, Product Category = Watches Sunglasses Jewellery/Watches/Men/Fashion, Brand Name = No Brand, Seller Name = OMGs, URL = https://www.daraz.pk/products/-i433228448-s2139698887.html?search=1, Price Details = Original: Rs. 2500, Discounted: Rs. 2199, Positive Seller Ratings = 96%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
    "\n",
    "\n",
    "Please provide a detailed answer to the question: Show me watches from sellers with more than 90% positive ratings..\n",
    "Your answer should integrate the essence of all three responses, providing a unified answer that leverages the diverse perspectives or data points provided by three responses.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Based on the given information, we can identify watches from sellers who have more than 90% positive ratings by analyzing the product details and seller ratings provided.\n",
    "For example, the product \"OMG's Stylish watch for men\" has a price of Rs. 2500 and an average rating of 96%. Additionally, the seller \"OMGs\" has received positive ratings of 96% on their website.\n",
    "On the other hand, the product \"High Quality Wrist Watch For Men & Boys\" has a price of Rs. 1000 and an average rating of 100%. The seller \"Maal-Lo\" has received negative ratings of 90% on their website.\n",
    "Therefore, based on the analysis, watches from sellers with more than 90% positive ratings include the \"OMG's Stylish watch for men\" and \"High Quality Wrist Watch For Men & Boys\". These watches are likely to be well-received by customers due to their high quality and stylish design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
