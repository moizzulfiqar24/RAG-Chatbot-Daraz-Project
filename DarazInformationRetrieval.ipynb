{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re, os\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchType = \"Main\"\n",
    "# search_query = \"What are your refund policies?\"\n",
    "\n",
    "searchType = \"Seller\"\n",
    "search_query = \"Can I make product bundles on Daraz?\"\n",
    "\n",
    "search_query = re.sub(r'\\bDaraz\\b\\s*', '', search_query, flags=re.IGNORECASE)\n",
    "chunkSize = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/outputCleaned.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/outputCleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paragraph_break(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "def is_unwanted_line(line):\n",
    "    # Check if a line ends with a colon\n",
    "    return line.strip().endswith(\":\")\n",
    "\n",
    "def process_files(folder_path, output_file):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    all_text = []\n",
    "    \n",
    "    for file in files:\n",
    "        current_paragraph = []\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_unwanted_line(line):\n",
    "                    continue  # Skip lines ending with a colon\n",
    "                if is_paragraph_break(line):\n",
    "                    if current_paragraph:\n",
    "                        all_text.append(\" \".join(current_paragraph))\n",
    "                        current_paragraph = []\n",
    "                else:\n",
    "                    # Remove leading/trailing whitespace and add the line to the current paragraph\n",
    "                    current_paragraph.append(line.strip())\n",
    "            # Don't forget to add the last paragraph if the file didn't end with a blank line\n",
    "            if current_paragraph:\n",
    "                all_text.append(\" \".join(current_paragraph))\n",
    "    \n",
    "    # Remove lines with less than 100 characters\n",
    "    all_text = [line for line in all_text if len(line) >= 100]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "if searchType.lower() == \"main\":\n",
    "    folder_path = '/Users/moiz/Moiz/Github/ITA-Project/DarazDataMain'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataMain.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataMain.txt\"\n",
    "elif searchType.lower() == \"seller\":\n",
    "    folder_path = '/Users/moiz/Moiz/Github/ITA-Project/DarazDataSeller'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataSeller.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataSeller.txt\"\n",
    "\n",
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can I make product bundles on ?\n",
      "\n",
      "Score: 0.6566495895385742\n",
      "Product Details: Why should I create bundles? Sellers can create bundles to differentiate their deals from competitors, especially in high-priced competitive categories It's a great opportunity to sell low-cost items as part of a bundle Bundles allow sellers to clear their inventory of whatever products they choose Additionally, bundles are a foolproof way to attract customers and increase conversion rate, sales and enhance overall performance.\n",
      "\n",
      "\n",
      "Score: 0.6418728828430176\n",
      "Product Details: What are the Benefits of creating Product Bundles? Sellers can create bundles to differentiate their deals from competitors, especially in high-priced competitive categories Itâ€™s a great opportunity to sell low cost items as part of a bundle Bundles allow Sellers to clear their inventory of whatever products they choose Additionally, bundles are a full proof way to attract customers and increase conversion rate, sales and enhance overall performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "search_products(search_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = search_query\n",
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def manage_chroma_folders(directory):\n",
    "    global chromaCounter\n",
    "    chromaCounter = 0  # Initialize the counter to zero\n",
    "\n",
    "    # Prepare to capture the highest number found in folder names starting with \"chromaDB\"\n",
    "    highest_number = 0\n",
    "\n",
    "    # Compile a regex pattern to match 'chromaDB' followed immediately by a number\n",
    "    pattern = re.compile(r'^chromaDB(\\d+)$')\n",
    "\n",
    "    # List all items in the directory\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    # First pass: Identify the highest number suffix for \"chromaDB\" folders\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            match = pattern.match(item)\n",
    "            if match:\n",
    "                # Extract the numeric part and update the highest_number if this one is greater\n",
    "                number = int(match.group(1))\n",
    "                if number > highest_number:\n",
    "                    highest_number = number\n",
    "\n",
    "    # Update the chromaCounter to the next available number\n",
    "    chromaCounter = highest_number + 1\n",
    "\n",
    "    # Second pass: Delete all folders starting with \"chroma\"\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(\"chroma\"):\n",
    "            full_path = os.path.join(directory, item)\n",
    "            shutil.rmtree(full_path)\n",
    "            # print(f\"Deleted folder: {full_path}\")\n",
    "\n",
    "# Usage example:\n",
    "directory = \"/Users/moiz/Moiz/Github/ITA-Project/\"\n",
    "manage_chroma_folders(directory)\n",
    "# print(f\"Next chromaCounter value: {chromaCounter}\")\n",
    "\n",
    "with open('chromaCounter.txt', 'w') as file:\n",
    "    file.write(f\"Current Chroma Counter = {chromaCounter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2262, which is longer than the specified 1500\n",
      "Created a chunk of size 7956, which is longer than the specified 1500\n",
      "Created a chunk of size 2841, which is longer than the specified 1500\n",
      "Created a chunk of size 2920, which is longer than the specified 1500\n",
      "Created a chunk of size 2399, which is longer than the specified 1500\n",
      "Created a chunk of size 1538, which is longer than the specified 1500\n",
      "Created a chunk of size 1731, which is longer than the specified 1500\n",
      "Created a chunk of size 2714, which is longer than the specified 1500\n",
      "Created a chunk of size 1561, which is longer than the specified 1500\n",
      "Created a chunk of size 1916, which is longer than the specified 1500\n",
      "Created a chunk of size 1642, which is longer than the specified 1500\n",
      "Created a chunk of size 2868, which is longer than the specified 1500\n",
      "Created a chunk of size 1639, which is longer than the specified 1500\n",
      "Created a chunk of size 1594, which is longer than the specified 1500\n",
      "Created a chunk of size 1813, which is longer than the specified 1500\n",
      "Created a chunk of size 2672, which is longer than the specified 1500\n",
      "Created a chunk of size 1679, which is longer than the specified 1500\n",
      "Created a chunk of size 2111, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker03 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"Can I make product bundles on ?\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    count = 1\n",
    "    for result in docs01:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    count = 1\n",
    "    for result in docs02:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    count = 1\n",
    "    for result in docs03:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove any image file references\n",
    "    text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "\n",
    "    # Normalize spacing issues\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Correct common typographical errors\n",
    "    text = re.sub(r\"isnot\", \"is not\", text)\n",
    "    text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "    text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "\n",
    "    # Remove redundant response indicators\n",
    "    text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed by a dot, e.g., \"1.\"\n",
    "    text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed directly by a dash, e.g., \"2-\"\n",
    "    text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "    # Deduplicate text\n",
    "    lines = text.split('.')\n",
    "    seen = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        if line_clean not in seen:\n",
    "            seen.add(line_clean)\n",
    "            unique_lines.append(line_clean)\n",
    "\n",
    "    # Reconstruct text with clean lines\n",
    "    cleaned_text = '. '.join(unique_lines).strip()\n",
    "    if not cleaned_text.endswith('.'):\n",
    "        cleaned_text += '.'\n",
    "\n",
    "    # Split into multiple lines with a reasonable width\n",
    "    wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "with open('output.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "    \n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "with open('outputCleaned.txt', 'w') as file:\n",
    "    file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"outputCleaned.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=200, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"Can I make product bundles on ?\"\n",
      "response1 = \"Bundling? Note: Only one type of bundle feature can be activated at a single time for a particular\n",
      "SKU. Minimum 5 customs products should be there if a seller is selling only custom made products\"\n",
      "response2 = \"What are the types of Product Bundling? Note: Only one type of bundle feature can be activated at a\n",
      "single time for a particular SKU. You must deactivate one bundle feature for that SKU in order to\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "count = 1\n",
    "for result in docs:\n",
    "    print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataMain.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataMain.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataSeller.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataSeller.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/output.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output.txt', 'r') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Img1 in Images Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
