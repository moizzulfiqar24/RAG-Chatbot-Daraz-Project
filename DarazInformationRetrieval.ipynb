{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re, os\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchType = \"Main\"\n",
    "search_query = \"What are your refund policies?\"\n",
    "\n",
    "# searchType = \"Seller\"\n",
    "# search_query = \"Can I make product bundles on Daraz?\"\n",
    "\n",
    "search_query = re.sub(r'\\bDaraz\\b\\s*', '', search_query, flags=re.IGNORECASE)\n",
    "chunkSize = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paragraph_break(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "def is_unwanted_line(line):\n",
    "    # Check if a line ends with a colon\n",
    "    return line.strip().endswith(\":\")\n",
    "\n",
    "def process_files(folder_path, output_file):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    all_text = []\n",
    "    \n",
    "    for file in files:\n",
    "        current_paragraph = []\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_unwanted_line(line):\n",
    "                    continue  # Skip lines ending with a colon\n",
    "                if is_paragraph_break(line):\n",
    "                    if current_paragraph:\n",
    "                        all_text.append(\" \".join(current_paragraph))\n",
    "                        current_paragraph = []\n",
    "                else:\n",
    "                    # Remove leading/trailing whitespace and add the line to the current paragraph\n",
    "                    current_paragraph.append(line.strip())\n",
    "            # Don't forget to add the last paragraph if the file didn't end with a blank line\n",
    "            if current_paragraph:\n",
    "                all_text.append(\" \".join(current_paragraph))\n",
    "    \n",
    "    # Remove lines with less than 100 characters\n",
    "    all_text = [line for line in all_text if len(line) >= 100]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "if searchType.lower() == \"main\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataMain.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataMain.txt\"\n",
    "elif searchType.lower() == \"seller\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataSeller.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataSeller.txt\"\n",
    "\n",
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are your refund policies?\n",
      "\n",
      "Score: 0.5706478953361511\n",
      "Product Details: 1.19 “Returns and Refunds Policy” shall mean the applicable Company policies which govern the procedure for returns and refunds of Products by Customers on the relevant Channels located at Returns and Refunds Policy of Pakistan.\n",
      "\n",
      "\n",
      "Score: 0.549439549446106\n",
      "Product Details: Home & Living Bedding & Bath, Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements, Household & Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind isnot applicable for return and refund. If the item received is damaged, defective, incorrect, or incomplete, a refund will be issued based on Daraz's assessment. Note: For device-related issues after usage or expiration of return policy period, please check if the item is covered under Seller orBrand Warranty. Refer to our Warranty Policy for information on the different warranty types and ways to contact the seller/manufacturer. Items that are non-returnable: Any custom-made items\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "search_products(search_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = search_query\n",
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def manage_chroma_folders(directory):\n",
    "    global chromaCounter\n",
    "    chromaCounter = 0  # Initialize the counter to zero\n",
    "\n",
    "    # Prepare to capture the highest number found in folder names starting with \"chromaDB\"\n",
    "    highest_number = 0\n",
    "\n",
    "    # Compile a regex pattern to match 'chromaDB' followed immediately by a number\n",
    "    pattern = re.compile(r'^chromaDB(\\d+)$')\n",
    "\n",
    "    # List all items in the directory\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    # First pass: Identify the highest number suffix for \"chromaDB\" folders\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            match = pattern.match(item)\n",
    "            if match:\n",
    "                # Extract the numeric part and update the highest_number if this one is greater\n",
    "                number = int(match.group(1))\n",
    "                if number > highest_number:\n",
    "                    highest_number = number\n",
    "\n",
    "    # Update the chromaCounter to the next available number\n",
    "    chromaCounter = highest_number + 1\n",
    "\n",
    "    # Second pass: Delete all folders starting with \"chroma\"\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(\"chroma\"):\n",
    "            full_path = os.path.join(directory, item)\n",
    "            shutil.rmtree(full_path)\n",
    "            # print(f\"Deleted folder: {full_path}\")\n",
    "\n",
    "# Usage example:\n",
    "directory = \"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/\"\n",
    "manage_chroma_folders(directory)\n",
    "# print(f\"Next chromaCounter value: {chromaCounter}\")\n",
    "\n",
    "with open('FinalWorking/chromaCounter.txt', 'w') as file:\n",
    "    file.write(f\"Current Chroma Counter = {chromaCounter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 26958, which is longer than the specified 1500\n",
      "Created a chunk of size 2500, which is longer than the specified 1500\n",
      "Created a chunk of size 2032, which is longer than the specified 1500\n",
      "Created a chunk of size 1537, which is longer than the specified 1500\n",
      "Created a chunk of size 1662, which is longer than the specified 1500\n",
      "Created a chunk of size 2178, which is longer than the specified 1500\n",
      "Created a chunk of size 1608, which is longer than the specified 1500\n",
      "Created a chunk of size 2486, which is longer than the specified 1500\n",
      "Created a chunk of size 2510, which is longer than the specified 1500\n",
      "Created a chunk of size 1754, which is longer than the specified 1500\n",
      "Created a chunk of size 3013, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2157, which is longer than the specified 1500\n",
      "Created a chunk of size 1692, which is longer than the specified 1500\n",
      "Created a chunk of size 1967, which is longer than the specified 1500\n",
      "Created a chunk of size 1830, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker03 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"What are your refund policies?\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    count = 1\n",
    "    for result in docs01:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    count = 1\n",
    "    for result in docs02:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    count = 1\n",
    "    for result in docs03:\n",
    "        file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove any image file references\n",
    "    text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "\n",
    "    # Normalize spacing issues\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Correct common typographical errors\n",
    "    text = re.sub(r\"isnot\", \"is not\", text)\n",
    "    text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "    text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "\n",
    "    # Remove redundant response indicators\n",
    "    text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed by a dot, e.g., \"1.\"\n",
    "    text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "    # Remove numbers followed directly by a dash, e.g., \"2-\"\n",
    "    text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "    # Deduplicate text\n",
    "    lines = text.split('.')\n",
    "    seen = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        if line_clean not in seen:\n",
    "            seen.add(line_clean)\n",
    "            unique_lines.append(line_clean)\n",
    "\n",
    "    # Reconstruct text with clean lines\n",
    "    cleaned_text = '. '.join(unique_lines).strip()\n",
    "    if not cleaned_text.endswith('.'):\n",
    "        cleaned_text += '.'\n",
    "\n",
    "    # Split into multiple lines with a reasonable width\n",
    "    wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "with open('output.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "    \n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "with open('outputCleaned.txt', 'w') as file:\n",
    "    file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"outputCleaned.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=200, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"What are your refund policies?\"\n",
      "response1 = \"and refund. Items that are non-returnable: Any custom-made items\" Home & Living Bedding & Bath,\n",
      "Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements, Household & Home Storage\"\n",
      "response2 = \"Home & Living Bedding & Bath, Furniture & Lighting, Kitchen & Dining, Home Décor, Home Improvements, Household & Home Storage Supplies, Lawn & Garden,Other Accessories Change of mind isnot applicable for return and refund. If the item received is damaged, defective, incorrect, or incomplete, a refund will be issued based on Daraz's assessment. Note: For device-related issues after usage or expiration of return policy period, please check if the item is covered under Seller orBrand Warranty. Refer to our Warranty Policy for information on the different warranty types and ways to contact the seller/manufacturer. Items that are non-returnable: Any custom-made items\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "count = 1\n",
    "for result in docs:\n",
    "    print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output.txt', 'r') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Img1 in Images Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
