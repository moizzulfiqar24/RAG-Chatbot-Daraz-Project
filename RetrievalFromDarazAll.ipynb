{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re, os, json, csv\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchType = \"Product\"\n",
    "# query = \"Can you list watches between Rs. 1000 and Rs. 2000?\"\n",
    "# query = \"Can you find Phone Cases that always ship on time?\"\n",
    "# query = \"Can you list Phone Cases under Rs. 1000?\"\n",
    "query = \"Show me tablet accessories with more than 90% positive ratings.\"\n",
    "\n",
    "# searchType = \"Main\"\n",
    "# query = \"What are your refund policies?\"\n",
    "\n",
    "# searchType = \"Seller\"\n",
    "# query = \"Can I make product bundles on Daraz?\"\n",
    "\n",
    "if query  != \"What is Daraz?\":\n",
    "    query = re.sub(r'\\bDaraz\\b\\s*', '', query, flags=re.IGNORECASE)\n",
    "    \n",
    "chunkSize = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "subjects = [\n",
    "    \"Phone Cases\", \"Power Banks\", \"iPhone Cables\", \"Android Cables\", \"Wall Chargers\",\n",
    "    \"Wireless Chargers\", \"Tablet Accessories\", \"Car Chargers\", \"Screen Protectors\",\n",
    "    \"Phone Camera Flash\", \"Lights\", \"Selfie Sticks\", \"Bluetooth Headphones\",\n",
    "    \"Wireless Earbuds\", \"Mono Headsets\", \"Headphones\", \"Wired Headsets\", \"Smartwatches\",\n",
    "    \"Fitness\", \"Trackers\", \"Fitness Tracker\", \"Virtual Reality\", \"Memory Cards\",\n",
    "    \"Lenses\", \"Tripods\", \"Monopods\", \"Camera Cases\", \"Camera\", \"Gimbals\", \"Batteries\",\n",
    "    \"Cooling Pads\", \"Keyboards\", \"Watches\"\n",
    "]\n",
    "\n",
    "headers = [\n",
    "    \"Product Number\", \"Product Name\", \"Product Category\", \"Brand Name\", \"Seller Name\", \n",
    "    \"Price Details\", \"Positive Seller Ratings\", \"Ship on Time\", \"Return Policy\"\n",
    "]\n",
    "\n",
    "def is_paragraph_break(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "def is_unwanted_line(line):\n",
    "    # Check if a line ends with a colon\n",
    "    return line.strip().endswith(\":\")\n",
    "\n",
    "def process_files(folder_path, output_file):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    all_text = []\n",
    "    \n",
    "    for file in files:\n",
    "        current_paragraph = []\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_unwanted_line(line):\n",
    "                    continue  # Skip lines ending with a colon\n",
    "                if is_paragraph_break(line):\n",
    "                    if current_paragraph:\n",
    "                        all_text.append(\" \".join(current_paragraph))\n",
    "                        current_paragraph = []\n",
    "                else:\n",
    "                    # Remove leading/trailing whitespace and add the line to the current paragraph\n",
    "                    current_paragraph.append(line.strip())\n",
    "            # Don't forget to add the last paragraph if the file didn't end with a blank line\n",
    "            if current_paragraph:\n",
    "                all_text.append(\" \".join(current_paragraph))\n",
    "    \n",
    "    # Remove lines with less than 100 characters\n",
    "    all_text = [line for line in all_text if len(line) >= 100]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "def normalize_subjects(subjects):\n",
    "    \"\"\"Lemmatize and normalize subjects for easier matching.\"\"\"\n",
    "    normalized_subjects = {}\n",
    "    for subject in subjects:\n",
    "        # Process the subject text with spaCy to lemmatize\n",
    "        doc = nlp(subject.lower())\n",
    "        # Join lemmatized words with hyphens\n",
    "        normalized = '-'.join([token.lemma_ for token in doc])\n",
    "        normalized_subjects[normalized] = subject  # Store original subject\n",
    "    return normalized_subjects\n",
    "\n",
    "def find_subject_in_query(query, subjects):\n",
    "    \"\"\"Find a subject in the lemmatized and normalized query.\"\"\"\n",
    "    normalized_subjects = normalize_subjects(subjects)\n",
    "    # Process the query text with spaCy to lemmatize\n",
    "    doc = nlp(query.lower())\n",
    "    lemmatized_query = '-'.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for normalized, original in normalized_subjects.items():\n",
    "        if normalized in lemmatized_query:\n",
    "            return original\n",
    "    return \"No subject found\"\n",
    "\n",
    "# Read each file in the directory\n",
    "def read_product_files(directory):\n",
    "    products_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = file.read()\n",
    "                corrected_data = '[' + data.replace('}\\n\\n{', '},\\n{') + ']'\n",
    "                try:\n",
    "                    product_info = json.loads(corrected_data)\n",
    "                    products_data.append(product_info)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "    return products_data\n",
    "\n",
    "# Extract product description specifically looking more robustly\n",
    "def extract_description(description_text):\n",
    "    # Attempt to extract the portion after \"Product Description\"\n",
    "    desc_start = description_text.find(\"Product Description:\")\n",
    "    if desc_start != -1:\n",
    "        # Extract starting from the found index through the end of the description\n",
    "        desc_substr = description_text[desc_start:]\n",
    "        desc_end = desc_substr.find(\"<br/>\")\n",
    "        if desc_end != -1:\n",
    "            return desc_substr[len(\"Product Description:\"):desc_end].strip()\n",
    "        else:\n",
    "            return desc_substr[len(\"Product Description:\"):].strip()\n",
    "    return \"Description not found.\"\n",
    "\n",
    "# Write the consolidated product info to an output file\n",
    "def write_product_info(products_data, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for i, product in enumerate(products_data, start=1):\n",
    "            product_dict = {}\n",
    "            for segment in product:\n",
    "                product_dict.update(segment)\n",
    "\n",
    "            product_name = product_dict.get(\"Product Name\", \"N/A\")\n",
    "            category_path = product_dict.get(\"Category\", \"N/A\").replace('\"', '')\n",
    "            brand_name = product_dict.get(\"Brand Name\", \"N/A\")\n",
    "            seller_name = product_dict.get(\"Seller Name\", \"N/A\")\n",
    "            url = product_dict.get(\"URL\", \"N/A\")\n",
    "            price_info = product_dict.get(\"Price Info\", [])\n",
    "            price_details = \" | \".join([f\"Original: {p[1]}, Discounted: {p[2]}\" for p in price_info])\n",
    "            # description = extract_description(product_dict.get(\"desc\", \"\").replace(\"<br/>\", \"\\n\"))\n",
    "            additional_info = product_dict.get(\"Additional Info\", {})\n",
    "            positive_ratings = additional_info.get(\"Positive Seller Ratings\", \"N/A\")\n",
    "            ship_on_time = additional_info.get(\"Ship on Time\", \"N/A\")\n",
    "            return_policy = product_dict.get(\"Return Policy\", {})\n",
    "            return_details = f\"{return_policy.get('Title', 'N/A')} ({return_policy.get('Subtitle', 'N/A')})\"\n",
    "\n",
    "            # product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Description = {description}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            outfile.write(product_entry)\n",
    "\n",
    "# Function to parse each line of the text file into structured data\n",
    "def parse_line(line):\n",
    "    # Prepare regex pattern with lookahead assertions to capture fields correctly\n",
    "    pattern = re.compile(\n",
    "        r\"Product Name = (?P<Product_Name>.*?)(?=, Product Category =)|\"\n",
    "        r\"Product Category = (?P<Product_Category>.*?)(?=, Brand Name =)|\"\n",
    "        r\"Brand Name = (?P<Brand_Name>.*?)(?=, Seller Name =)|\"\n",
    "        r\"Seller Name = (?P<Seller_Name>.*?)(?=, URL =)|\"\n",
    "        r\"Price Details = (?P<Price_Details>.*?)(?=, Positive Seller Ratings =)|\"\n",
    "        r\"Positive Seller Ratings = (?P<Positive_Seller_Ratings>.*?)(?=, Ship on Time =)|\"\n",
    "        r\"Ship on Time = (?P<Ship_on_Time>.*?)(?=, Return Policy =)|\"\n",
    "        r\"Return Policy = (?P<Return_Policy>.*?)(?=, Product \\d+:|, URL =|$)\"\n",
    "    )\n",
    "\n",
    "    # Extract product number separately\n",
    "    product_number = re.match(r\"Product (\\d+):\", line).group(1)\n",
    "\n",
    "    # Find all matches in the line\n",
    "    matches = pattern.finditer(line)\n",
    "    data = {k: v for m in matches for k, v in m.groupdict().items() if v is not None}\n",
    "\n",
    "    # Constructing the row based on required headers\n",
    "    return [\n",
    "        \"Product \" + product_number,\n",
    "        data.get(\"Product_Name\", \"\"),\n",
    "        data.get(\"Product_Category\", \"\"),\n",
    "        data.get(\"Brand_Name\", \"\"),\n",
    "        data.get(\"Seller_Name\", \"\"),\n",
    "        data.get(\"Price_Details\", \"\"),\n",
    "        data.get(\"Positive_Seller_Ratings\", \"\"),\n",
    "        data.get(\"Ship_on_Time\", \"\"),\n",
    "        data.get(\"Return_Policy\", \"\")\n",
    "    ]\n",
    "\n",
    "def extract_info_simple(query):\n",
    "    # Define keywords for subject identification\n",
    "    subject_keywords = [\"watch\", \"watches\", \"smartwatch\", \"luxury watch\"]\n",
    "    brand_names = products_df['Brand Name'].str.lower().unique().tolist()\n",
    "    seller_names = products_df['Seller Name'].str.lower().unique().tolist()\n",
    "\n",
    "    # Patterns for limitations\n",
    "    price_pattern = r\"Rs\\.\\s*\\d+|\\d+\\s*%|between\\s*Rs\\.\\s*\\d+\\s*and\\s*Rs\\.\\s*\\d+\"\n",
    "    # Updated rating pattern to be more specific and catch contexts like \"more than 90%\"\n",
    "    rating_pattern = r\"more than \\d{1,3}% positive ratings|less than \\d{1,3}% positive ratings|\\d{1,3}% positive ratings|\\d{1,3}%\"\n",
    "    time_pattern = r\"ship on time\"\n",
    "    \n",
    "    # Find subjects\n",
    "    subjects = [keyword for keyword in subject_keywords if keyword in query.lower()]\n",
    "    subjects.extend([brand for brand in brand_names if brand in query.lower()])\n",
    "    \n",
    "    # Find limitations\n",
    "    limitations = re.findall(price_pattern, query)\n",
    "    limitations.extend(re.findall(rating_pattern, query))\n",
    "    if \"top-rated sellers\" in query.lower() or \"highly rated sellers\" in query.lower():\n",
    "        limitations.append(\"top-rated sellers\")\n",
    "    if re.search(time_pattern, query, re.IGNORECASE):\n",
    "        limitations.append(\"ship on time\")\n",
    "\n",
    "    # Check if there are specific seller names mentioned\n",
    "    for seller in seller_names:\n",
    "        if seller in query.lower():\n",
    "            limitations.append(f\"sold by {seller}\")\n",
    "\n",
    "    # return {\"subjects\": subjects, \"limitations\": limitations}\n",
    "    return limitations\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load the product data from a CSV file and preprocess it.\"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Discounted Price'] = data['Price Details'].apply(\n",
    "        lambda x: min(map(int, re.findall(r'Discounted: Rs\\. (\\d+)', x)))\n",
    "    )\n",
    "    data['Positive Seller Ratings'] = data['Positive Seller Ratings'].str.rstrip('%').astype(int)\n",
    "    data['Ship on Time'] = data['Ship on Time'].str.rstrip('%').astype(int)\n",
    "    return data\n",
    "\n",
    "def parse_limitation(limitation):\n",
    "    \"\"\"Parse the limitation string into a structured dictionary.\"\"\"\n",
    "    if 'between Rs.' in limitation:\n",
    "        low, high = map(int, re.findall(r'\\d+', limitation))\n",
    "        return {'price_range': (low, high)}\n",
    "    elif 'Rs.' in limitation:\n",
    "        price = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'price_exact': price}\n",
    "    elif 'sold by' in limitation:\n",
    "        seller = limitation.split('sold by ')[1].strip()\n",
    "        return {'seller_name': seller}\n",
    "    elif 'top-rated sellers' in limitation:\n",
    "        return {'top_rated_sellers': 90}\n",
    "    elif '%' in limitation:\n",
    "        rating = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'top_rated_sellers': rating}\n",
    "    elif 'ship on time' in limitation:\n",
    "        return {'ship_on_time': 100}\n",
    "    else:\n",
    "        return None  # Handle unrecognized input\n",
    "\n",
    "def filter_productsTwo(data, limitation_dict):\n",
    "    \"\"\"Apply filters to the data based on parsed limitations.\"\"\"\n",
    "    if limitation_dict is None:\n",
    "        return []\n",
    "    key, value = next(iter(limitation_dict.items()))\n",
    "    if key == 'price_exact':\n",
    "        filtered_data = data[data['Discounted Price'] == value]\n",
    "    elif key == 'price_range':\n",
    "        filtered_data = data[(data['Discounted Price'] >= value[0]) & (data['Discounted Price'] <= value[1])]\n",
    "    elif key == 'seller_name':\n",
    "        filtered_data = data[data['Seller Name'].str.contains(value, case=False, na=False)]\n",
    "    elif key == 'top_rated_sellers':\n",
    "        filtered_data = data[data['Positive Seller Ratings'] >= value]\n",
    "    elif key == 'ship_on_time':\n",
    "        filtered_data = data[data['Ship on Time'] == value]\n",
    "    return filtered_data['Product Number'].tolist()\n",
    "\n",
    "def filter_products(input_filename, output_filename, matching_product_numbers):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Prepare to collect matching lines\n",
    "    matching_lines = []\n",
    "\n",
    "    # Filter lines based on matching_product_numbers\n",
    "    if matching_product_numbers:\n",
    "        # Create a set for faster lookup\n",
    "        product_set = set(matching_product_numbers)\n",
    "        for line in lines:\n",
    "            # Assuming each line starts with a product identifier like \"Product XX:\"\n",
    "            product_number = line.split(':', 1)[0].strip()\n",
    "            if product_number in product_set:\n",
    "                matching_lines.append(line)\n",
    "    else:\n",
    "        # If matching_product_numbers is empty, select all lines\n",
    "        matching_lines = lines\n",
    "\n",
    "    # Write the selected lines to the output file\n",
    "    with open(output_filename, 'w') as file:\n",
    "        file.writelines(matching_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Function to load data, handle NaN values, and save back to CSV\n",
    "# def load_dataTwo(filepath, output_filepath):\n",
    "#     # Load data\n",
    "#     data = pd.read_csv(filepath)\n",
    "\n",
    "#     # Iterate through each column in DataFrame\n",
    "#     for column in data.columns:\n",
    "#         # Check data type of the column\n",
    "#         if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n",
    "#             # For numerical columns, fill NaNs with the mean of the column\n",
    "#             data[column].fillna(data[column].mean(), inplace=True)\n",
    "#         else:\n",
    "#             # For categorical columns, fill NaNs with the mode of the column (most frequent value)\n",
    "#             mode_value = data[column].mode()[0]\n",
    "#             data[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "#     # Save the modified DataFrame back to a new CSV file\n",
    "#     data.to_csv(output_filepath, index=False)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # Specify the path for the output file\n",
    "# output_file_path = 'ProcessedFinalProductsList.csv'\n",
    "\n",
    "# # Use the function to load your data and save it to a new CSV\n",
    "# data = load_dataTwo('FinalProductsList.csv', output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "products/Tablet-Accessories\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() == \"main\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataMain.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataMain.txt\"\n",
    "elif searchType.lower() == \"seller\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataSeller.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataSeller.txt\"\n",
    "elif searchType.lower() == \"product\":  \n",
    "    result = find_subject_in_query(query, subjects)\n",
    "    words = result.split()\n",
    "    result = '-'.join(words) if len(words) > 1 else result\n",
    "    directory_path = 'products/' + str(result)\n",
    "    print(directory_path)\n",
    "    products_data = read_product_files(directory_path)\n",
    "    output_file = 'FinalProductsList.txt'\n",
    "    write_product_info(products_data, output_file)\n",
    "    input_file_path = 'FinalProductsList.txt'\n",
    "    output_csv_path = 'FinalProductsList.csv'\n",
    "    # Reading the text file and writing to CSV\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file, \\\n",
    "        open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)  # Writing headers to the CSV file\n",
    "        \n",
    "        for line in file:\n",
    "            if line.strip():  # Ensuring the line has content\n",
    "                row = parse_line(line)\n",
    "                writer.writerow(row)  # Writing the parsed data as a row in the CSV file\n",
    "\n",
    "    products_df = pd.read_csv('FinalProductsList.csv')\n",
    "\n",
    "    # # changes\n",
    "    products_df.replace('N/A', np.nan, inplace=True)\n",
    "    numeric_cols = products_df.select_dtypes(include=[np.number]).columns\n",
    "    products_df[numeric_cols] = products_df[numeric_cols].fillna(products_df[numeric_cols].mean())\n",
    "    # # till here \n",
    "\n",
    "    # Function to load data, handle NaN values, and save back to CSV\n",
    "    def load_dataTwo(filepath, output_filepath):\n",
    "        # Load data\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        # Iterate through each column in DataFrame\n",
    "        for column in data.columns:\n",
    "            # Check data type of the column\n",
    "            if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n",
    "                # For numerical columns, fill NaNs with the mean of the column\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                # For categorical columns, fill NaNs with the mode of the column (most frequent value)\n",
    "                mode_value = data[column].mode()[0]\n",
    "                data[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "        # Save the modified DataFrame back to a new CSV file\n",
    "        data.to_csv(output_filepath, index=False)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Specify the path for the output file\n",
    "    output_file_path = 'ProcessedFinalProductsList.csv'\n",
    "\n",
    "    # Use the function to load your data and save it to a new CSV\n",
    "    data = load_dataTwo('FinalProductsList.csv', output_file_path)\n",
    "\n",
    "    extracted_info = extract_info_simple(query)\n",
    "\n",
    "    limitation = \"', '\".join(extracted_info)\n",
    "    limitation = f\"'{limitation}'\"\n",
    "\n",
    "    # data = pd.read_csv('FinalProductsList.csv')\n",
    "    # data.fillna({'your_integer_column': 0}, inplace=True)\n",
    "\n",
    "    \n",
    "    # data = load_data('FinalProductsList.csv')\n",
    "    data = load_data('ProcessedFinalProductsList.csv')\n",
    "\n",
    "\n",
    "\n",
    "    # Example limitation\n",
    "    # limitation = \"'90%', 'more than 90% positive ratings'\"\n",
    "\n",
    "    # Parse and filter products based on the limitation\n",
    "    parsed_limitation = parse_limitation(limitation)\n",
    "\n",
    "    # print(parsed_limitation)\n",
    "    matching_product_numbers = filter_productsTwo(data, parsed_limitation)\n",
    "\n",
    "    filter_products('FinalProductsList.txt', 'ProductsList.txt', matching_product_numbers)\n",
    "\n",
    "    file_name = \"ProductsList.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Show me tablet accessories with more than 90% positive ratings.\n",
      "\n",
      "Score: 0.5748814344406128\n",
      "Product Details: Product 03: Product Name = inifinix smart8 pro tpu case (high quality), Product Category = Mobiles & Tablets/Tablet Accessories/Cases & Covers, Brand Name = No Brand, Seller Name = RA Acessories, URL = https://www.daraz.pk/products/8-tpu-i488054690-s2295749424.html?search=1, Price Details = Original: Rs. 250, Discounted: Rs. 200, Positive Seller Ratings = 100%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
      "\n",
      "\n",
      "Score: 0.5393520593643188\n",
      "Product Details: Product 49: Product Name = For iPad 7th|8th|9th Generation Case,Compatible iPad 10.2 inch (2019|2020|2021 Releases),iPad Case 10.2 Case with Pencil Holder, Lightweight Smart Cover with Soft TPU Back,Auto Sleep|Wake, Product Category = Mobiles & Tablets/Tablet Accessories/Cases & Covers, Brand Name = No Brand, Seller Name = ePower Store, URL = https://www.daraz.pk/products/7th8th9th-102-2019-20202021-tpu-i450166765-s2286154223.html?search=1, Price Details = Original: Rs. 2499, Discounted: Rs. 1899, Positive Seller Ratings = 93%, Ship on Time = 99%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "search_products(query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def manage_chroma_folders(directory):\n",
    "    global chromaCounter\n",
    "    chromaCounter = 0  # Initialize the counter to zero\n",
    "\n",
    "    # Prepare to capture the highest number found in folder names starting with \"chromaDB\"\n",
    "    highest_number = 0\n",
    "\n",
    "    # Compile a regex pattern to match 'chromaDB' followed immediately by a number\n",
    "    pattern = re.compile(r'^chromaDB(\\d+)$')\n",
    "\n",
    "    # List all items in the directory\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    # First pass: Identify the highest number suffix for \"chromaDB\" folders\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            match = pattern.match(item)\n",
    "            if match:\n",
    "                # Extract the numeric part and update the highest_number if this one is greater\n",
    "                number = int(match.group(1))\n",
    "                if number > highest_number:\n",
    "                    highest_number = number\n",
    "\n",
    "    # Update the chromaCounter to the next available number\n",
    "    chromaCounter = highest_number + 1\n",
    "\n",
    "    # Second pass: Delete all folders starting with \"chroma\"\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(\"chroma\"):\n",
    "            full_path = os.path.join(directory, item)\n",
    "            shutil.rmtree(full_path)\n",
    "            # print(f\"Deleted folder: {full_path}\")\n",
    "\n",
    "# Usage example:\n",
    "directory = \"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/\"\n",
    "manage_chroma_folders(directory)\n",
    "# print(f\"Next chromaCounter value: {chromaCounter}\")\n",
    "\n",
    "with open('FinalWorking/chromaCounter.txt', 'w') as file:\n",
    "    file.write(f\"Current Chroma Counter = {chromaCounter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    chunker03 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=GPT4AllEmbeddings(),\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"Show me tablet accessories with more than 90% positive ratings.\"\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() == \"product\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        for result in docs01:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "        for result in docs02:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "        # for result in docs03:\n",
    "        #     file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "    def remove_duplicates(input_file, output_file):\n",
    "        try:\n",
    "            # Read all lines from the input file\n",
    "            with open(input_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            # Remove duplicate lines, maintaining order\n",
    "            unique_lines = []\n",
    "            seen = set()\n",
    "            for line in lines:\n",
    "                if line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "            \n",
    "            # Write the unique lines to the output file\n",
    "            with open(output_file, 'w') as file:\n",
    "                file.writelines(unique_lines)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Example usage\n",
    "    remove_duplicates('output.txt', 'outputCleaned.txt')\n",
    "\n",
    "    # os.rename(\"output.txt\", \"outputCleaned.txt\")\n",
    "\n",
    "    # def clean_and_deduplicate_file(filepath):\n",
    "    # # Read the file and filter lines\n",
    "    #     with open(filepath, 'r') as file:\n",
    "    #         lines = file.readlines()\n",
    "        \n",
    "    #     # Filter lines that start with \"Product\"\n",
    "    #     filtered_lines = [line for line in lines if line.startswith('Product')]\n",
    "        \n",
    "    #     # Remove duplicates by converting the list to a set and back to a list\n",
    "    #     unique_lines = list(set(filtered_lines))\n",
    "\n",
    "    #     # Write the unique lines back to the file\n",
    "    #     with open(filepath, 'w') as file:\n",
    "    #         file.writelines(unique_lines)\n",
    "\n",
    "    # # Path to the file\n",
    "    # file_path = 'outputCleaned.txt'\n",
    "    # # Clean and deduplicate the file\n",
    "    # clean_and_deduplicate_file(file_path)\n",
    "\n",
    "    # remove_duplicates('outputCleaned.txt', 'outputCleaned.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() == \"main\" or searchType.lower() == \"seller\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        count = 1\n",
    "        for result in docs01:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs02:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs03:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Remove any image file references\n",
    "        text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "\n",
    "        # Normalize spacing issues\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Correct common typographical errors\n",
    "        text = re.sub(r\"isnot\", \"is not\", text)\n",
    "        text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "        text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "\n",
    "        # Remove redundant response indicators\n",
    "        text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "\n",
    "        # Remove numbers followed by a dot, e.g., \"1.\"\n",
    "        text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "        # Remove numbers followed directly by a dash, e.g., \"2-\"\n",
    "        text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "        # Deduplicate text\n",
    "        lines = text.split('.')\n",
    "        seen = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            line_clean = line.strip()\n",
    "            if line_clean not in seen:\n",
    "                seen.add(line_clean)\n",
    "                unique_lines.append(line_clean)\n",
    "\n",
    "        # Reconstruct text with clean lines\n",
    "        cleaned_text = '. '.join(unique_lines).strip()\n",
    "        if not cleaned_text.endswith('.'):\n",
    "            cleaned_text += '.'\n",
    "\n",
    "        # Split into multiple lines with a reasonable width\n",
    "        wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "        return wrapped_text\n",
    "\n",
    "    with open('output.txt', 'r') as file:\n",
    "        raw_text = file.read()\n",
    "        \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    with open('outputCleaned.txt', 'w') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "    def summarize_text(file_path, language=\"english\", summary_length=10):\n",
    "        # Create a parser for the given text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "        \n",
    "        # Initialize the LSA summarizer\n",
    "        summarizer = LsaSummarizer()\n",
    "        \n",
    "        # Summarize the text, specifying the number of sentences in the summary\n",
    "        summary = summarizer(parser.document, summary_length)\n",
    "        \n",
    "        # Convert summarized sentences back to text\n",
    "        summarized_text = ' '.join(str(sentence) for sentence in summary)\n",
    "        \n",
    "        # Write the summary back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summarized_text)\n",
    "\n",
    "    # Path to the text file\n",
    "    file_path = 'outputCleaned.txt'\n",
    "    # Function call to summarize the text\n",
    "    summarize_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 805, which is longer than the specified 200\n",
      "Created a chunk of size 462, which is longer than the specified 200\n",
      "Created a chunk of size 725, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"outputCleaned.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=200, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"Show me tablet accessories with more than 90% positive ratings.\"\n",
      "response1 = \"Product 03: Product Name = inifinix smart8 pro tpu case (high quality), Product Category = Mobiles & Tablets/Tablet Accessories/Cases & Covers, Brand Name = No Brand, Seller Name = RA Acessories, URL = https://www.daraz.pk/products/8-tpu-i488054690-s2295749424.html?search=1, Price Details = Original: Rs. 250, Discounted: Rs. 200, Positive Seller Ratings = 100%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\"\n",
      "response2 = \"Product 02: Product Name = ARMICO Dual Charging Modes Stylus Pen for iPad with Palm Rejection Tilt Sensitivity Active Touch Screen Apple Pencil Compatible with 2018-2023 Apple iPad, Product Category = Mobiles & Tablets/Tablet Accessories/Stylus Pens, Brand Name = No Brand, Seller Name = ARMICO 3C Life, URL = https://www.daraz.pk/products/armico-dual-charging-modes-stylus-pen-for-ipad-with-palm-rejection-tilt-sensitivity-active-touch-screen-apple-pencil-compatible-with-2018-2023-apple-ipad-i447908725-s2137620320.html?search=1, Price Details = Original: Rs. 4569, Discounted: Rs. 4459 | Original: Rs. 4299, Discounted: Rs. 4109 | Original: Rs. 1999, Discounted: Rs. 1049, Positive Seller Ratings = 92%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
      "Product 03: Product Name = inifinix smart8 pro tpu case (high quality), Product Category = Mobiles & Tablets/Tablet Accessories/Cases & Covers, Brand Name = No Brand, Seller Name = RA Acessories, URL = https://www.daraz.pk/products/8-tpu-i488054690-s2295749424.html?search=1, Price Details = Original: Rs. 250, Discounted: Rs. 200, Positive Seller Ratings = 100%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\"\n"
     ]
    }
   ],
   "source": [
    "print(\"question = \\\"\" + query + \"\\\"\")\n",
    "count = 1\n",
    "for result in docs:\n",
    "    print(f\"response\" + str(count) + \" = \\\"\" + result[0].page_content + \"\\\"\")  \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProcessedFinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProcessedFinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProductsList.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output.txt', 'r') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Img1 in Images Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
