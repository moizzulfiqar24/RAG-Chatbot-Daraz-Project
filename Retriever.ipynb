{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Importing Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re, os, json, csv\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Define Query & Search Type\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchType = \"Product\"\n",
    "query = \"Can you list watches between Rs. 1000 and Rs. 2000?\"\n",
    "# # query = \"Can you find wall chargers that always ship on time?\"\n",
    "# query = \"Show me tablet accessories with more than 90% positive ratings.\"\n",
    "\n",
    "# searchType = \"Main\"\n",
    "# # query = \"What are your refund policies?\"\n",
    "# query = \"What are discount option available for HBL card users?\"\n",
    "\n",
    "# searchType = \"Seller\"\n",
    "# query = \"Can I make product bundles on Daraz?\"\n",
    "# # query = \"What are your refund policies?\"\n",
    "\n",
    "if query  != \"What is Daraz?\":\n",
    "    query = re.sub(r'\\bDaraz\\b\\s*', '', query, flags=re.IGNORECASE)\n",
    "    \n",
    "chunkSize = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/outputCleaned.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/outputCleaned.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleanedSummarised.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleanedSummarised.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Define Required Functions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "subjects = [\n",
    "    \"Phone Cases\", \"Power Banks\", \"iPhone Cables\", \"Android Cables\", \"Wall Chargers\",\n",
    "    \"Wireless Chargers\", \"Tablet Accessories\", \"Car Chargers\", \"Screen Protectors\",\n",
    "    \"Phone Camera Flash\", \"Lights\", \"Selfie Sticks\", \"Bluetooth Headphones\",\n",
    "    \"Wireless Earbuds\", \"Mono Headsets\", \"Headphones\", \"Wired Headsets\", \"Smartwatches\",\n",
    "    \"Fitness\", \"Trackers\", \"Fitness Tracker\", \"Virtual Reality\", \"Memory Cards\",\n",
    "    \"Lenses\", \"Tripods\", \"Monopods\", \"Camera Cases\", \"Camera\", \"Gimbals\", \"Batteries\",\n",
    "    \"Cooling Pads\", \"Keyboards\", \"Watches\"\n",
    "]\n",
    "\n",
    "headers = [\n",
    "    \"Product Number\", \"Product Name\", \"Product Category\", \"Brand Name\", \"Seller Name\", \n",
    "    \"Price Details\", \"Positive Seller Ratings\", \"Ship on Time\", \"Return Policy\"\n",
    "]\n",
    "\n",
    "def is_paragraph_break(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "def is_unwanted_line(line):\n",
    "    return line.strip().endswith(\":\")\n",
    "\n",
    "def process_files(folder_path, output_file):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    all_text = []\n",
    "    \n",
    "    for file in files:\n",
    "        current_paragraph = []\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_unwanted_line(line):\n",
    "                    continue  \n",
    "                if is_paragraph_break(line):\n",
    "                    if current_paragraph:\n",
    "                        all_text.append(\" \".join(current_paragraph))\n",
    "                        current_paragraph = []\n",
    "                else:\n",
    "                    current_paragraph.append(line.strip())\n",
    "            if current_paragraph:\n",
    "                all_text.append(\" \".join(current_paragraph))\n",
    "    \n",
    "    all_text = [line for line in all_text if len(line) >= 100]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "def normalize_subjects(subjects):\n",
    "    \"\"\"Lemmatize and normalize subjects for easier matching.\"\"\"\n",
    "    normalized_subjects = {}\n",
    "    for subject in subjects:\n",
    "        doc = nlp(subject.lower())\n",
    "        normalized = '-'.join([token.lemma_ for token in doc])\n",
    "        normalized_subjects[normalized] = subject  \n",
    "    return normalized_subjects\n",
    "\n",
    "def find_subject_in_query(query, subjects):\n",
    "    \"\"\"Find a subject in the lemmatized and normalized query.\"\"\"\n",
    "    normalized_subjects = normalize_subjects(subjects)\n",
    "    doc = nlp(query.lower())\n",
    "    lemmatized_query = '-'.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for normalized, original in normalized_subjects.items():\n",
    "        if normalized in lemmatized_query:\n",
    "            return original\n",
    "    return \"No subject found\"\n",
    "\n",
    "def read_product_files(directory):\n",
    "    products_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = file.read()\n",
    "                corrected_data = '[' + data.replace('}\\n\\n{', '},\\n{') + ']'\n",
    "                try:\n",
    "                    product_info = json.loads(corrected_data)\n",
    "                    products_data.append(product_info)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "    return products_data\n",
    "\n",
    "def extract_description(description_text):\n",
    "    desc_start = description_text.find(\"Product Description:\")\n",
    "    if desc_start != -1:\n",
    "        desc_substr = description_text[desc_start:]\n",
    "        desc_end = desc_substr.find(\"<br/>\")\n",
    "        if desc_end != -1:\n",
    "            return desc_substr[len(\"Product Description:\"):desc_end].strip()\n",
    "        else:\n",
    "            return desc_substr[len(\"Product Description:\"):].strip()\n",
    "    return \"Description not found.\"\n",
    "\n",
    "def write_product_info(products_data, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for i, product in enumerate(products_data, start=1):\n",
    "            product_dict = {}\n",
    "            for segment in product:\n",
    "                product_dict.update(segment)\n",
    "\n",
    "            product_name = product_dict.get(\"Product Name\", \"N/A\")\n",
    "            category_path = product_dict.get(\"Category\", \"N/A\").replace('\"', '')\n",
    "            brand_name = product_dict.get(\"Brand Name\", \"N/A\")\n",
    "            seller_name = product_dict.get(\"Seller Name\", \"N/A\")\n",
    "            url = product_dict.get(\"URL\", \"N/A\")\n",
    "            price_info = product_dict.get(\"Price Info\", [])\n",
    "            price_details = \" | \".join([f\"Original: {p[1]}, Discounted: {p[2]}\" for p in price_info])\n",
    "            additional_info = product_dict.get(\"Additional Info\", {})\n",
    "            positive_ratings = additional_info.get(\"Positive Seller Ratings\", \"N/A\")\n",
    "            ship_on_time = additional_info.get(\"Ship on Time\", \"N/A\")\n",
    "            return_policy = product_dict.get(\"Return Policy\", {})\n",
    "            return_details = f\"{return_policy.get('Title', 'N/A')} ({return_policy.get('Subtitle', 'N/A')})\"\n",
    "\n",
    "            # product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Description = {description}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            outfile.write(product_entry)\n",
    "\n",
    "def parse_line(line):\n",
    "    pattern = re.compile(\n",
    "        r\"Product Name = (?P<Product_Name>.*?)(?=, Product Category =)|\"\n",
    "        r\"Product Category = (?P<Product_Category>.*?)(?=, Brand Name =)|\"\n",
    "        r\"Brand Name = (?P<Brand_Name>.*?)(?=, Seller Name =)|\"\n",
    "        r\"Seller Name = (?P<Seller_Name>.*?)(?=, URL =)|\"\n",
    "        r\"Price Details = (?P<Price_Details>.*?)(?=, Positive Seller Ratings =)|\"\n",
    "        r\"Positive Seller Ratings = (?P<Positive_Seller_Ratings>.*?)(?=, Ship on Time =)|\"\n",
    "        r\"Ship on Time = (?P<Ship_on_Time>.*?)(?=, Return Policy =)|\"\n",
    "        r\"Return Policy = (?P<Return_Policy>.*?)(?=, Product \\d+:|, URL =|$)\"\n",
    "    )\n",
    "\n",
    "    product_number = re.match(r\"Product (\\d+):\", line).group(1)\n",
    "\n",
    "    matches = pattern.finditer(line)\n",
    "    data = {k: v for m in matches for k, v in m.groupdict().items() if v is not None}\n",
    "\n",
    "    return [\n",
    "        \"Product \" + product_number,\n",
    "        data.get(\"Product_Name\", \"\"),\n",
    "        data.get(\"Product_Category\", \"\"),\n",
    "        data.get(\"Brand_Name\", \"\"),\n",
    "        data.get(\"Seller_Name\", \"\"),\n",
    "        data.get(\"Price_Details\", \"\"),\n",
    "        data.get(\"Positive_Seller_Ratings\", \"\"),\n",
    "        data.get(\"Ship_on_Time\", \"\"),\n",
    "        data.get(\"Return_Policy\", \"\")\n",
    "    ]\n",
    "\n",
    "def extract_info_simple(query):\n",
    "    subject_keywords = [\"watch\", \"watches\", \"smartwatch\", \"luxury watch\"]\n",
    "    brand_names = products_df['Brand Name'].str.lower().unique().tolist()\n",
    "    seller_names = products_df['Seller Name'].str.lower().unique().tolist()\n",
    "\n",
    "    price_pattern = r\"Rs\\.\\s*\\d+|\\d+\\s*%|between\\s*Rs\\.\\s*\\d+\\s*and\\s*Rs\\.\\s*\\d+\"\n",
    "    rating_pattern = r\"more than \\d{1,3}% positive ratings|less than \\d{1,3}% positive ratings|\\d{1,3}% positive ratings|\\d{1,3}%\"\n",
    "    time_pattern = r\"ship on time\"\n",
    "    \n",
    "    # Find subjects\n",
    "    subjects = [keyword for keyword in subject_keywords if keyword in query.lower()]\n",
    "    subjects.extend([brand for brand in brand_names if brand in query.lower()])\n",
    "    \n",
    "    # Find limitations\n",
    "    limitations = re.findall(price_pattern, query)\n",
    "    limitations.extend(re.findall(rating_pattern, query))\n",
    "    if \"top-rated sellers\" in query.lower() or \"highly rated sellers\" in query.lower():\n",
    "        limitations.append(\"top-rated sellers\")\n",
    "    if re.search(time_pattern, query, re.IGNORECASE):\n",
    "        limitations.append(\"ship on time\")\n",
    "\n",
    "    for seller in seller_names:\n",
    "        if seller in query.lower():\n",
    "            limitations.append(f\"sold by {seller}\")\n",
    "\n",
    "    # return {\"subjects\": subjects, \"limitations\": limitations}\n",
    "    return limitations\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load the product data from a CSV file and preprocess it.\"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Discounted Price'] = data['Price Details'].apply(\n",
    "        lambda x: min(map(int, re.findall(r'Discounted: Rs\\. (\\d+)', x)))\n",
    "    )\n",
    "    data['Positive Seller Ratings'] = data['Positive Seller Ratings'].str.rstrip('%').astype(int)\n",
    "    data['Ship on Time'] = data['Ship on Time'].str.rstrip('%').astype(int)\n",
    "    return data\n",
    "\n",
    "def parse_limitation(limitation):\n",
    "    \"\"\"Parse the limitation string into a structured dictionary.\"\"\"\n",
    "    if 'between Rs.' in limitation:\n",
    "        low, high = map(int, re.findall(r'\\d+', limitation))\n",
    "        return {'price_range': (low, high)}\n",
    "    elif 'Rs.' in limitation:\n",
    "        price = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'price_exact': price}\n",
    "    elif 'sold by' in limitation:\n",
    "        seller = limitation.split('sold by ')[1].strip()\n",
    "        return {'seller_name': seller}\n",
    "    elif 'top-rated sellers' in limitation:\n",
    "        return {'top_rated_sellers': 90}\n",
    "    elif '%' in limitation:\n",
    "        rating = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'top_rated_sellers': rating}\n",
    "    elif 'ship on time' in limitation:\n",
    "        return {'ship_on_time': 100}\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "def filter_productsTwo(data, limitation_dict):\n",
    "    \"\"\"Apply filters to the data based on parsed limitations.\"\"\"\n",
    "    if limitation_dict is None:\n",
    "        return []\n",
    "    key, value = next(iter(limitation_dict.items()))\n",
    "    if key == 'price_exact':\n",
    "        filtered_data = data[data['Discounted Price'] == value]\n",
    "    elif key == 'price_range':\n",
    "        filtered_data = data[(data['Discounted Price'] >= value[0]) & (data['Discounted Price'] <= value[1])]\n",
    "    elif key == 'seller_name':\n",
    "        filtered_data = data[data['Seller Name'].str.contains(value, case=False, na=False)]\n",
    "    elif key == 'top_rated_sellers':\n",
    "        filtered_data = data[data['Positive Seller Ratings'] >= value]\n",
    "    elif key == 'ship_on_time':\n",
    "        filtered_data = data[data['Ship on Time'] == value]\n",
    "    return filtered_data['Product Number'].tolist()\n",
    "\n",
    "def filter_products(input_filename, output_filename, matching_product_numbers):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    matching_lines = []\n",
    "\n",
    "    if matching_product_numbers:\n",
    "        product_set = set(matching_product_numbers)\n",
    "        for line in lines:\n",
    "            product_number = line.split(':', 1)[0].strip()\n",
    "            if product_number in product_set:\n",
    "                matching_lines.append(line)\n",
    "    else:\n",
    "        matching_lines = lines\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        file.writelines(matching_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Defining Directories Based on Query Type\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Products/Watches\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() == \"main\":\n",
    "    folder_path = '/Users/moiz/Moiz/Github/ITA-Project/Data/DarazDataMain'  \n",
    "    output_file = 'DarazDataMain.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataMain.txt\"\n",
    "elif searchType.lower() == \"seller\":\n",
    "    folder_path = '/Users/moiz/Moiz/Github/ITA-Project/Data/DarazDataSeller'  \n",
    "    output_file = 'DarazDataSeller.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataSeller.txt\"\n",
    "elif searchType.lower() == \"product\":  \n",
    "    result = find_subject_in_query(query, subjects)\n",
    "    words = result.split()\n",
    "    result = '-'.join(words) if len(words) > 1 else result\n",
    "    directory_path = 'Data/Products/' + str(result)\n",
    "    print(directory_path)\n",
    "    products_data = read_product_files(directory_path)\n",
    "    output_file = 'FinalProductsList.txt'\n",
    "    write_product_info(products_data, output_file)\n",
    "    input_file_path = 'FinalProductsList.txt'\n",
    "    output_csv_path = 'FinalProductsList.csv'\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file, \\\n",
    "        open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers) \n",
    "        \n",
    "        for line in file:\n",
    "            if line.strip(): \n",
    "                row = parse_line(line)\n",
    "                writer.writerow(row) \n",
    "\n",
    "    products_df = pd.read_csv('FinalProductsList.csv')\n",
    "\n",
    "    products_df.replace('N/A', np.nan, inplace=True)\n",
    "    numeric_cols = products_df.select_dtypes(include=[np.number]).columns\n",
    "    products_df[numeric_cols] = products_df[numeric_cols].fillna(products_df[numeric_cols].mean())\n",
    "\n",
    "    def load_dataTwo(filepath, output_filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        for column in data.columns:\n",
    "            if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                mode_value = data[column].mode()[0]\n",
    "                data[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "        data.to_csv(output_filepath, index=False)\n",
    "\n",
    "        return data\n",
    "\n",
    "    output_file_path = 'ProcessedFinalProductsList.csv'\n",
    "\n",
    "    data = load_dataTwo('FinalProductsList.csv', output_file_path)\n",
    "\n",
    "    extracted_info = extract_info_simple(query)\n",
    "\n",
    "    limitation = \"', '\".join(extracted_info)\n",
    "    limitation = f\"'{limitation}'\"\n",
    "\n",
    "    data = load_data('ProcessedFinalProductsList.csv')\n",
    "\n",
    "    parsed_limitation = parse_limitation(limitation)\n",
    "\n",
    "    matching_product_numbers = filter_productsTwo(data, parsed_limitation)\n",
    "\n",
    "    filter_products('FinalProductsList.txt', 'ProductsList.txt', matching_product_numbers)\n",
    "\n",
    "    file_name = \"ProductsList.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Creating Sentence Embeddings\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you list watches between Rs. 1000 and Rs. 2000?\n",
      "\n",
      "Score: 0.5335537791252136\n",
      "Product Details: Product 04: Product Name = Men Quartz Watch for Waterproof Vacuum Plating Strap with Car Wheel Rim Hub Design Sport Automatic Wristwatch (Does not rotate), Product Category = Watches Sunglasses Jewellery/Watches/Men/Fashion, Brand Name = No Brand, Seller Name = D&K Online, URL = https://www.daraz.pk/products/-i411043073-s1964553711.html?search=1, Price Details = Original: Rs. 3999, Discounted: Rs. 1768 | Original: Rs. 3999, Discounted: Rs. 1869 | Original: Rs. 3999, Discounted: Rs. 1769 | Original: Rs. 3999, Discounted: Rs. 1768 | Original: Rs. 3999, Discounted: Rs. 1868, Positive Seller Ratings = 82%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
      "\n",
      "\n",
      "Score: 0.4519554078578949\n",
      "Product Details: Product 10: Product Name = 2022 trend full-automatic quartz movement men's watch wheel non mechanical watch wrist watch fashion men's watch, Product Category = Watches Sunglasses Jewellery/Watches/Men/Fashion, Brand Name = No Brand, Seller Name = lamgool Electronic, URL = https://www.daraz.pk/products/2022-i366103747-s1819200832.html?search=1, Price Details = Original: Rs. 1836, Discounted: Rs. 1410, Positive Seller Ratings = 81%, Ship on Time = 100%, Return Policy = 14 days free & easy return (Change of mind is not applicable)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "search_products(query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Initializing Vector DB\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def manage_chroma_folders(directory):\n",
    "    global chromaCounter\n",
    "    chromaCounter = 0 \n",
    "\n",
    "    highest_number = 0\n",
    "\n",
    "    pattern = re.compile(r'^chromaDB(\\d+)$')\n",
    "\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            match = pattern.match(item)\n",
    "            if match:\n",
    "                number = int(match.group(1))\n",
    "                if number > highest_number:\n",
    "                    highest_number = number\n",
    "\n",
    "    chromaCounter = highest_number + 1\n",
    "\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(\"chroma\"):\n",
    "            full_path = os.path.join(directory, item)\n",
    "            shutil.rmtree(full_path)\n",
    "\n",
    "directory = \"/Users/moiz/Moiz/Github/ITA-Project\"\n",
    "manage_chroma_folders(directory)\n",
    "\n",
    "with open('chromaCounter.txt', 'w') as file:\n",
    "    file.write(f\"Current Chroma Counter = {chromaCounter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 01 = Character Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 02 = Recursive Character Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 03 = NLTK Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    chunker03 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=GPT4AllEmbeddings(),\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Combining into one collective file. Cleaning & Removing Duplicates Then\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"Can you list watches between Rs. 1000 and Rs. 2000?\"\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() == \"product\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        for result in docs01:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "        for result in docs02:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "    def remove_duplicates(input_file, output_file):\n",
    "        try:\n",
    "            with open(input_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            unique_lines = []\n",
    "            seen = set()\n",
    "            for line in lines:\n",
    "                if line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "            \n",
    "            with open(output_file, 'w') as file:\n",
    "                file.writelines(unique_lines)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    remove_duplicates('output.txt', 'outputCleaned.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() == \"main\" or searchType.lower() == \"seller\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        count = 1\n",
    "        for result in docs01:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs02:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs03:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        text = re.sub(r\"isnot\", \"is not\", text)\n",
    "        text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "        text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "        text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "        text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "        text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "        # Deduplicate text\n",
    "        lines = text.split('.')\n",
    "        seen = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            line_clean = line.strip()\n",
    "            if line_clean not in seen:\n",
    "                seen.add(line_clean)\n",
    "                unique_lines.append(line_clean)\n",
    "\n",
    "        # Reconstruct text with clean lines\n",
    "        cleaned_text = '. '.join(unique_lines).strip()\n",
    "        if not cleaned_text.endswith('.'):\n",
    "            cleaned_text += '.'\n",
    "\n",
    "        # Split into multiple lines with a reasonable width\n",
    "        wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "        return wrapped_text\n",
    "\n",
    "    with open('output.txt', 'r') as file:\n",
    "        raw_text = file.read()\n",
    "        \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    with open('outputCleaned.txt', 'w') as file:\n",
    "        file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Applying Extractive Summary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "    def summarize_text(file_path, language=\"english\", summary_length=10):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "        \n",
    "        summarizer = LsaSummarizer()\n",
    "        \n",
    "        summary = summarizer(parser.document, summary_length)\n",
    "        \n",
    "        summarized_text = ' '.join(str(sentence) for sentence in summary)\n",
    "        \n",
    "        with open(\"outputCleanedSummarised.txt\", 'w', encoding='utf-8') as file:\n",
    "            file.write(summarized_text)\n",
    "\n",
    "    file_path = 'outputCleaned.txt'\n",
    "    summarize_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Cleaning Non-Important Files\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataMain.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataMain.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataSeller.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/DarazDataSeller.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/output.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/output.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/FinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/FinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/ProcessedFinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/ProcessedFinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/FinalProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/FinalProductsList.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Moiz/Github/ITA-Project/ProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Moiz/Github/ITA-Project/ProductsList.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
