{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Importing Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import re, os, json, csv\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Define Query & Search Type\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchType = \"Product\"\n",
    "# # query = \"Can you list watches between Rs. 1000 and Rs. 2000?\"\n",
    "# # query = \"Can you find Phone Cases that always ship on time?\"\n",
    "# # query = \"Can you list Phone Cases under Rs. 1000?\"\n",
    "# query = \"Show me tablet accessories with more than 90% positive ratings.\"\n",
    "\n",
    "searchType = \"Main\"\n",
    "# query = \"What are your refund policies?\"\n",
    "query = \"What are discount option available for HBL card users?\"\n",
    "\n",
    "# searchType = \"Seller\"\n",
    "# query = \"Can I make product bundles on Daraz?\"\n",
    "\n",
    "if query  != \"What is Daraz?\":\n",
    "    query = re.sub(r'\\bDaraz\\b\\s*', '', query, flags=re.IGNORECASE)\n",
    "    \n",
    "chunkSize = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleaned.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleanedSummarised.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/outputCleanedSummarised.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Define Required Functions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "subjects = [\n",
    "    \"Phone Cases\", \"Power Banks\", \"iPhone Cables\", \"Android Cables\", \"Wall Chargers\",\n",
    "    \"Wireless Chargers\", \"Tablet Accessories\", \"Car Chargers\", \"Screen Protectors\",\n",
    "    \"Phone Camera Flash\", \"Lights\", \"Selfie Sticks\", \"Bluetooth Headphones\",\n",
    "    \"Wireless Earbuds\", \"Mono Headsets\", \"Headphones\", \"Wired Headsets\", \"Smartwatches\",\n",
    "    \"Fitness\", \"Trackers\", \"Fitness Tracker\", \"Virtual Reality\", \"Memory Cards\",\n",
    "    \"Lenses\", \"Tripods\", \"Monopods\", \"Camera Cases\", \"Camera\", \"Gimbals\", \"Batteries\",\n",
    "    \"Cooling Pads\", \"Keyboards\", \"Watches\"\n",
    "]\n",
    "\n",
    "headers = [\n",
    "    \"Product Number\", \"Product Name\", \"Product Category\", \"Brand Name\", \"Seller Name\", \n",
    "    \"Price Details\", \"Positive Seller Ratings\", \"Ship on Time\", \"Return Policy\"\n",
    "]\n",
    "\n",
    "def is_paragraph_break(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "def is_unwanted_line(line):\n",
    "    return line.strip().endswith(\":\")\n",
    "\n",
    "def process_files(folder_path, output_file):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    all_text = []\n",
    "    \n",
    "    for file in files:\n",
    "        current_paragraph = []\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_unwanted_line(line):\n",
    "                    continue  \n",
    "                if is_paragraph_break(line):\n",
    "                    if current_paragraph:\n",
    "                        all_text.append(\" \".join(current_paragraph))\n",
    "                        current_paragraph = []\n",
    "                else:\n",
    "                    current_paragraph.append(line.strip())\n",
    "            if current_paragraph:\n",
    "                all_text.append(\" \".join(current_paragraph))\n",
    "    \n",
    "    all_text = [line for line in all_text if len(line) >= 100]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "def normalize_subjects(subjects):\n",
    "    \"\"\"Lemmatize and normalize subjects for easier matching.\"\"\"\n",
    "    normalized_subjects = {}\n",
    "    for subject in subjects:\n",
    "        doc = nlp(subject.lower())\n",
    "        normalized = '-'.join([token.lemma_ for token in doc])\n",
    "        normalized_subjects[normalized] = subject  \n",
    "    return normalized_subjects\n",
    "\n",
    "def find_subject_in_query(query, subjects):\n",
    "    \"\"\"Find a subject in the lemmatized and normalized query.\"\"\"\n",
    "    normalized_subjects = normalize_subjects(subjects)\n",
    "    doc = nlp(query.lower())\n",
    "    lemmatized_query = '-'.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for normalized, original in normalized_subjects.items():\n",
    "        if normalized in lemmatized_query:\n",
    "            return original\n",
    "    return \"No subject found\"\n",
    "\n",
    "def read_product_files(directory):\n",
    "    products_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = file.read()\n",
    "                corrected_data = '[' + data.replace('}\\n\\n{', '},\\n{') + ']'\n",
    "                try:\n",
    "                    product_info = json.loads(corrected_data)\n",
    "                    products_data.append(product_info)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "    return products_data\n",
    "\n",
    "def extract_description(description_text):\n",
    "    desc_start = description_text.find(\"Product Description:\")\n",
    "    if desc_start != -1:\n",
    "        desc_substr = description_text[desc_start:]\n",
    "        desc_end = desc_substr.find(\"<br/>\")\n",
    "        if desc_end != -1:\n",
    "            return desc_substr[len(\"Product Description:\"):desc_end].strip()\n",
    "        else:\n",
    "            return desc_substr[len(\"Product Description:\"):].strip()\n",
    "    return \"Description not found.\"\n",
    "\n",
    "def write_product_info(products_data, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for i, product in enumerate(products_data, start=1):\n",
    "            product_dict = {}\n",
    "            for segment in product:\n",
    "                product_dict.update(segment)\n",
    "\n",
    "            product_name = product_dict.get(\"Product Name\", \"N/A\")\n",
    "            category_path = product_dict.get(\"Category\", \"N/A\").replace('\"', '')\n",
    "            brand_name = product_dict.get(\"Brand Name\", \"N/A\")\n",
    "            seller_name = product_dict.get(\"Seller Name\", \"N/A\")\n",
    "            url = product_dict.get(\"URL\", \"N/A\")\n",
    "            price_info = product_dict.get(\"Price Info\", [])\n",
    "            price_details = \" | \".join([f\"Original: {p[1]}, Discounted: {p[2]}\" for p in price_info])\n",
    "            additional_info = product_dict.get(\"Additional Info\", {})\n",
    "            positive_ratings = additional_info.get(\"Positive Seller Ratings\", \"N/A\")\n",
    "            ship_on_time = additional_info.get(\"Ship on Time\", \"N/A\")\n",
    "            return_policy = product_dict.get(\"Return Policy\", {})\n",
    "            return_details = f\"{return_policy.get('Title', 'N/A')} ({return_policy.get('Subtitle', 'N/A')})\"\n",
    "\n",
    "            # product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Description = {description}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            product_entry = f\"Product {i:02d}: Product Name = {product_name}, Product Category = {category_path}, Brand Name = {brand_name}, Seller Name = {seller_name}, URL = {url}, Price Details = {price_details}, Positive Seller Ratings = {positive_ratings}, Ship on Time = {ship_on_time}, Return Policy = {return_details}\\n\"\n",
    "            outfile.write(product_entry)\n",
    "\n",
    "def parse_line(line):\n",
    "    pattern = re.compile(\n",
    "        r\"Product Name = (?P<Product_Name>.*?)(?=, Product Category =)|\"\n",
    "        r\"Product Category = (?P<Product_Category>.*?)(?=, Brand Name =)|\"\n",
    "        r\"Brand Name = (?P<Brand_Name>.*?)(?=, Seller Name =)|\"\n",
    "        r\"Seller Name = (?P<Seller_Name>.*?)(?=, URL =)|\"\n",
    "        r\"Price Details = (?P<Price_Details>.*?)(?=, Positive Seller Ratings =)|\"\n",
    "        r\"Positive Seller Ratings = (?P<Positive_Seller_Ratings>.*?)(?=, Ship on Time =)|\"\n",
    "        r\"Ship on Time = (?P<Ship_on_Time>.*?)(?=, Return Policy =)|\"\n",
    "        r\"Return Policy = (?P<Return_Policy>.*?)(?=, Product \\d+:|, URL =|$)\"\n",
    "    )\n",
    "\n",
    "    product_number = re.match(r\"Product (\\d+):\", line).group(1)\n",
    "\n",
    "    matches = pattern.finditer(line)\n",
    "    data = {k: v for m in matches for k, v in m.groupdict().items() if v is not None}\n",
    "\n",
    "    return [\n",
    "        \"Product \" + product_number,\n",
    "        data.get(\"Product_Name\", \"\"),\n",
    "        data.get(\"Product_Category\", \"\"),\n",
    "        data.get(\"Brand_Name\", \"\"),\n",
    "        data.get(\"Seller_Name\", \"\"),\n",
    "        data.get(\"Price_Details\", \"\"),\n",
    "        data.get(\"Positive_Seller_Ratings\", \"\"),\n",
    "        data.get(\"Ship_on_Time\", \"\"),\n",
    "        data.get(\"Return_Policy\", \"\")\n",
    "    ]\n",
    "\n",
    "def extract_info_simple(query):\n",
    "    subject_keywords = [\"watch\", \"watches\", \"smartwatch\", \"luxury watch\"]\n",
    "    brand_names = products_df['Brand Name'].str.lower().unique().tolist()\n",
    "    seller_names = products_df['Seller Name'].str.lower().unique().tolist()\n",
    "\n",
    "    price_pattern = r\"Rs\\.\\s*\\d+|\\d+\\s*%|between\\s*Rs\\.\\s*\\d+\\s*and\\s*Rs\\.\\s*\\d+\"\n",
    "    rating_pattern = r\"more than \\d{1,3}% positive ratings|less than \\d{1,3}% positive ratings|\\d{1,3}% positive ratings|\\d{1,3}%\"\n",
    "    time_pattern = r\"ship on time\"\n",
    "    \n",
    "    # Find subjects\n",
    "    subjects = [keyword for keyword in subject_keywords if keyword in query.lower()]\n",
    "    subjects.extend([brand for brand in brand_names if brand in query.lower()])\n",
    "    \n",
    "    # Find limitations\n",
    "    limitations = re.findall(price_pattern, query)\n",
    "    limitations.extend(re.findall(rating_pattern, query))\n",
    "    if \"top-rated sellers\" in query.lower() or \"highly rated sellers\" in query.lower():\n",
    "        limitations.append(\"top-rated sellers\")\n",
    "    if re.search(time_pattern, query, re.IGNORECASE):\n",
    "        limitations.append(\"ship on time\")\n",
    "\n",
    "    for seller in seller_names:\n",
    "        if seller in query.lower():\n",
    "            limitations.append(f\"sold by {seller}\")\n",
    "\n",
    "    # return {\"subjects\": subjects, \"limitations\": limitations}\n",
    "    return limitations\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load the product data from a CSV file and preprocess it.\"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Discounted Price'] = data['Price Details'].apply(\n",
    "        lambda x: min(map(int, re.findall(r'Discounted: Rs\\. (\\d+)', x)))\n",
    "    )\n",
    "    data['Positive Seller Ratings'] = data['Positive Seller Ratings'].str.rstrip('%').astype(int)\n",
    "    data['Ship on Time'] = data['Ship on Time'].str.rstrip('%').astype(int)\n",
    "    return data\n",
    "\n",
    "def parse_limitation(limitation):\n",
    "    \"\"\"Parse the limitation string into a structured dictionary.\"\"\"\n",
    "    if 'between Rs.' in limitation:\n",
    "        low, high = map(int, re.findall(r'\\d+', limitation))\n",
    "        return {'price_range': (low, high)}\n",
    "    elif 'Rs.' in limitation:\n",
    "        price = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'price_exact': price}\n",
    "    elif 'sold by' in limitation:\n",
    "        seller = limitation.split('sold by ')[1].strip()\n",
    "        return {'seller_name': seller}\n",
    "    elif 'top-rated sellers' in limitation:\n",
    "        return {'top_rated_sellers': 90}\n",
    "    elif '%' in limitation:\n",
    "        rating = int(re.findall(r'\\d+', limitation)[0])\n",
    "        return {'top_rated_sellers': rating}\n",
    "    elif 'ship on time' in limitation:\n",
    "        return {'ship_on_time': 100}\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "def filter_productsTwo(data, limitation_dict):\n",
    "    \"\"\"Apply filters to the data based on parsed limitations.\"\"\"\n",
    "    if limitation_dict is None:\n",
    "        return []\n",
    "    key, value = next(iter(limitation_dict.items()))\n",
    "    if key == 'price_exact':\n",
    "        filtered_data = data[data['Discounted Price'] == value]\n",
    "    elif key == 'price_range':\n",
    "        filtered_data = data[(data['Discounted Price'] >= value[0]) & (data['Discounted Price'] <= value[1])]\n",
    "    elif key == 'seller_name':\n",
    "        filtered_data = data[data['Seller Name'].str.contains(value, case=False, na=False)]\n",
    "    elif key == 'top_rated_sellers':\n",
    "        filtered_data = data[data['Positive Seller Ratings'] >= value]\n",
    "    elif key == 'ship_on_time':\n",
    "        filtered_data = data[data['Ship on Time'] == value]\n",
    "    return filtered_data['Product Number'].tolist()\n",
    "\n",
    "def filter_products(input_filename, output_filename, matching_product_numbers):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    matching_lines = []\n",
    "\n",
    "    if matching_product_numbers:\n",
    "        product_set = set(matching_product_numbers)\n",
    "        for line in lines:\n",
    "            product_number = line.split(':', 1)[0].strip()\n",
    "            if product_number in product_set:\n",
    "                matching_lines.append(line)\n",
    "    else:\n",
    "        matching_lines = lines\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        file.writelines(matching_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Defining Directories Based on Query Type\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() == \"main\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataMain.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataMain.txt\"\n",
    "elif searchType.lower() == \"seller\":\n",
    "    folder_path = '/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller'  # Change this to the path of your folder\n",
    "    output_file = 'DarazDataSeller.txt'  \n",
    "    process_files(folder_path, output_file)\n",
    "    file_name = \"DarazDataSeller.txt\"\n",
    "elif searchType.lower() == \"product\":  \n",
    "    result = find_subject_in_query(query, subjects)\n",
    "    words = result.split()\n",
    "    result = '-'.join(words) if len(words) > 1 else result\n",
    "    directory_path = 'products/' + str(result)\n",
    "    print(directory_path)\n",
    "    products_data = read_product_files(directory_path)\n",
    "    output_file = 'FinalProductsList.txt'\n",
    "    write_product_info(products_data, output_file)\n",
    "    input_file_path = 'FinalProductsList.txt'\n",
    "    output_csv_path = 'FinalProductsList.csv'\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file, \\\n",
    "        open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers) \n",
    "        \n",
    "        for line in file:\n",
    "            if line.strip(): \n",
    "                row = parse_line(line)\n",
    "                writer.writerow(row) \n",
    "\n",
    "    products_df = pd.read_csv('FinalProductsList.csv')\n",
    "\n",
    "    products_df.replace('N/A', np.nan, inplace=True)\n",
    "    numeric_cols = products_df.select_dtypes(include=[np.number]).columns\n",
    "    products_df[numeric_cols] = products_df[numeric_cols].fillna(products_df[numeric_cols].mean())\n",
    "\n",
    "    def load_dataTwo(filepath, output_filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        for column in data.columns:\n",
    "            if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                mode_value = data[column].mode()[0]\n",
    "                data[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "        data.to_csv(output_filepath, index=False)\n",
    "\n",
    "        return data\n",
    "\n",
    "    output_file_path = 'ProcessedFinalProductsList.csv'\n",
    "\n",
    "    data = load_dataTwo('FinalProductsList.csv', output_file_path)\n",
    "\n",
    "    extracted_info = extract_info_simple(query)\n",
    "\n",
    "    limitation = \"', '\".join(extracted_info)\n",
    "    limitation = f\"'{limitation}'\"\n",
    "\n",
    "    data = load_data('ProcessedFinalProductsList.csv')\n",
    "\n",
    "    parsed_limitation = parse_limitation(limitation)\n",
    "\n",
    "    matching_product_numbers = filter_productsTwo(data, parsed_limitation)\n",
    "\n",
    "    filter_products('FinalProductsList.txt', 'ProductsList.txt', matching_product_numbers)\n",
    "\n",
    "    file_name = \"ProductsList.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Creating Sentence Embeddings\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are discount option available for HBL card users?\n",
      "\n",
      "Score: 0.5994541049003601\n",
      "Product Details: What are the weekly payment partner discounts? Wednesday - HBL 10% off, 900 cap, 2 transactions/card, Debit/Credit Thursday - UBL 10% off, 1000 cap, 1 transaction/card, Debit/Credit Saturday - Askari Bank 10% off, 1000 cap, 3 transactions/card, Credit Sunday - Standard Chartered 10% off, 1000 cap, 2 transactions/card, Credit\n",
      "\n",
      "\n",
      "Score: 0.5042264461517334\n",
      "Product Details: How to Collect Vouchers during a Live stream? pasted image 3.png Look for the pop-up voucher tab displaying the discount offer. Click on the tab, which will redirect you to the voucher section. Review the terms and conditions and check the minimum spend requirement, if any. To Check how to redeem them please check What are Daraz Vouchers and how to use them?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    products = f.readlines()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_embeddings = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "def search_products(query, k):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, product_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=k)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"\\nScore:\", score.item())\n",
    "        print(\"Product Details:\", products[idx])\n",
    "\n",
    "search_products(query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(file_name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Initializing Vector DB\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def manage_chroma_folders(directory):\n",
    "    global chromaCounter\n",
    "    chromaCounter = 0 \n",
    "\n",
    "    highest_number = 0\n",
    "\n",
    "    pattern = re.compile(r'^chromaDB(\\d+)$')\n",
    "\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            match = pattern.match(item)\n",
    "            if match:\n",
    "                number = int(match.group(1))\n",
    "                if number > highest_number:\n",
    "                    highest_number = number\n",
    "\n",
    "    chromaCounter = highest_number + 1\n",
    "\n",
    "    for item in items:\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(\"chroma\"):\n",
    "            full_path = os.path.join(directory, item)\n",
    "            shutil.rmtree(full_path)\n",
    "\n",
    "directory = \"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/\"\n",
    "manage_chroma_folders(directory)\n",
    "\n",
    "with open('FinalWorking/chromaCounter.txt', 'w') as file:\n",
    "    file.write(f\"Current Chroma Counter = {chromaCounter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 01 = Character Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 26958, which is longer than the specified 1500\n",
      "Created a chunk of size 2500, which is longer than the specified 1500\n",
      "Created a chunk of size 2032, which is longer than the specified 1500\n",
      "Created a chunk of size 1537, which is longer than the specified 1500\n",
      "Created a chunk of size 1662, which is longer than the specified 1500\n",
      "Created a chunk of size 2178, which is longer than the specified 1500\n",
      "Created a chunk of size 1608, which is longer than the specified 1500\n",
      "Created a chunk of size 2486, which is longer than the specified 1500\n",
      "Created a chunk of size 2510, which is longer than the specified 1500\n",
      "Created a chunk of size 1754, which is longer than the specified 1500\n",
      "Created a chunk of size 3013, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "chunker01 = \"Fixed-size (in characters) Overlapping Sliding Window\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs01 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 02 = Recursive Character Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker02 = \"Recursive Structure Aware Splitting\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"], chunk_size=chunkSize, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "docs02 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Chunker 03 = NLTK Text Splitter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2183, which is longer than the specified 1500\n",
      "Created a chunk of size 1718, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1967, which is longer than the specified 1500\n",
      "Created a chunk of size 1830, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    chunker03 = \"NLP Chunking: Tracking Topic Changes\"\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(chunk_size=chunkSize)#, separator=\"\\n\")\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    persist_directory = 'chromaDB' + str(chromaCounter) + '/'\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=GPT4AllEmbeddings(),\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    docs03 = vectordb.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Combining into one collective file. Cleaning & Removing Duplicates Then\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() == \"product\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        for result in docs01:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "        for result in docs02:\n",
    "            file.write(result[0].page_content+ \"\\n\")\n",
    "\n",
    "    def remove_duplicates(input_file, output_file):\n",
    "        try:\n",
    "            with open(input_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            unique_lines = []\n",
    "            seen = set()\n",
    "            for line in lines:\n",
    "                if line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "            \n",
    "            with open(output_file, 'w') as file:\n",
    "                file.writelines(unique_lines)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    remove_duplicates('output.txt', 'outputCleaned.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = \"What are discount option available for HBL card users?\"\n"
     ]
    }
   ],
   "source": [
    "if searchType.lower() == \"main\" or searchType.lower() == \"seller\":\n",
    "    print(\"question = \\\"\" + query + \"\\\"\")\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        count = 1\n",
    "        for result in docs01:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs02:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        count = 1\n",
    "        for result in docs03:\n",
    "            file.write(f\"response{count} = \\\"{result[0].page_content}\\\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"\\S+\\.(png|jpg|jpeg|gif)\\s*\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        text = re.sub(r\"isnot\", \"is not\", text)\n",
    "        text = re.sub(r\"orBrand\", \"or Brand\", text)\n",
    "        text = re.sub(r\"ourWarranty\", \"our Warranty\", text)\n",
    "        text = re.sub(r\"response\\d+\\s*=\\s*\\\"\", \"\", text)\n",
    "        text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "        text = re.sub(r\"\\d+-\", \"\", text)\n",
    "\n",
    "        # Deduplicate text\n",
    "        lines = text.split('.')\n",
    "        seen = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            line_clean = line.strip()\n",
    "            if line_clean not in seen:\n",
    "                seen.add(line_clean)\n",
    "                unique_lines.append(line_clean)\n",
    "\n",
    "        # Reconstruct text with clean lines\n",
    "        cleaned_text = '. '.join(unique_lines).strip()\n",
    "        if not cleaned_text.endswith('.'):\n",
    "            cleaned_text += '.'\n",
    "\n",
    "        # Split into multiple lines with a reasonable width\n",
    "        wrapped_text = textwrap.fill(cleaned_text, width=100)\n",
    "\n",
    "        return wrapped_text\n",
    "\n",
    "    with open('output.txt', 'r') as file:\n",
    "        raw_text = file.read()\n",
    "        \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    with open('outputCleaned.txt', 'w') as file:\n",
    "        file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Applying Extractive Summary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if searchType.lower() != \"product\":\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "    def summarize_text(file_path, language=\"english\", summary_length=10):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "        \n",
    "        summarizer = LsaSummarizer()\n",
    "        \n",
    "        summary = summarizer(parser.document, summary_length)\n",
    "        \n",
    "        summarized_text = ' '.join(str(sentence) for sentence in summary)\n",
    "        \n",
    "        with open(\"outputCleanedSummarised.txt\", 'w', encoding='utf-8') as file:\n",
    "            file.write(summarized_text)\n",
    "\n",
    "    file_path = 'outputCleaned.txt'\n",
    "    summarize_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<!-- <b>Tip:</b> Use blue boxes (alert-info) for tips and notes.  -->\n",
    "Cleaning Non-Important Files\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataMain.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/DarazDataSeller.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/output.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProcessedFinalProductsList.csv\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProcessedFinalProductsList.csv\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/FinalProductsList.txt\")\n",
    "\n",
    "if os.path.exists(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProductsList.txt\"):\n",
    "    os.remove(\"/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/6th - Spring 2024/ITA/Project/ProductsList.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
